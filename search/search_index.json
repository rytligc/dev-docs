{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my docs Mkdocs For full documentation of Mkdocs visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Extensions Tabs ``` c \"=== \" C \" ``` c #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } ``` === \"C++\" ``` c ++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } ``` Result C C++ 1 2 3 4 5 6 #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } 1 2 3 4 5 6 #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } InlineHilite Here is some code: import pymdownx ; pymdownx . __version__ . 1 `#!py3 import pymdownx; pymdownx.__version__`. The mock shebang will be treated like text here: #!js var test = 0; . Tab 1 Tab 2 Markdown content . Multiple paragraphs. More Markdown content . list item a list item b Tab A Tab B Different tab set. 1 More content.","title":"Welcome to my docs"},{"location":"#welcome-to-my-docs","text":"","title":"Welcome to my docs"},{"location":"#mkdocs","text":"For full documentation of Mkdocs visit mkdocs.org .","title":"Mkdocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#extensions","text":"","title":"Extensions"},{"location":"#tabs","text":"``` c \"=== \" C \" ``` c #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } ``` === \"C++\" ``` c ++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } ```","title":"Tabs"},{"location":"#result","text":"C C++ 1 2 3 4 5 6 #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } 1 2 3 4 5 6 #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; }","title":"Result"},{"location":"#inlinehilite","text":"Here is some code: import pymdownx ; pymdownx . __version__ . 1 `#!py3 import pymdownx; pymdownx.__version__`. The mock shebang will be treated like text here: #!js var test = 0; . Tab 1 Tab 2 Markdown content . Multiple paragraphs. More Markdown content . list item a list item b Tab A Tab B Different tab set. 1 More content.","title":"InlineHilite"},{"location":"CKA/AdditionalResources/","text":"Useful resources Official docs Unofficial K8s docs KodeKloud K8s docs Linux foundation exam tips Digi Hunch Exam walkthrough Study Guide Tips And Tricks","title":"Useful resources"},{"location":"CKA/AdditionalResources/#useful-resources","text":"Official docs Unofficial K8s docs KodeKloud K8s docs Linux foundation exam tips Digi Hunch Exam walkthrough Study Guide Tips And Tricks","title":"Useful resources"},{"location":"CKA/CertTips/","text":"Certification tips and cheat sheet commands Generally use kubectl describe pod to verify that everything is working as expected. Setup by Linux Foundation At the start of each task you'll be provided with the command to ensure you are on the correct cluster to complete the task. SSH to any node: ssh <nodename> Sudo on any given node: sudo -i You can also use sudo to execute commands with elevated privileges at any time You must return to the base node (hostname node-1) after completing each task. Nested\u2212ssh is not supported. You can use kubectl and the appropriate context to work on any cluster from the base node. When connected to a cluster member via ssh, you will only be able to work on that particular cluster via kubectl. The base system and the cluster nodes have pre-installed and pre-configured: kubectl with k alias and Bash autocompletion jq for YAML/JSON processing tmux for terminal multiplexing curl and wget for testing web services man and man pages for further documentation Further instructions for connecting to cluster nodes will be provided in the appropriate tasks Where no explicit namespace is specified, the default namespace should be acted upon. If you need to destroy/recreate a resource to perform a certain task, it is your responsibility to back up the resource definition appropriately prior to destroying the resource. Imperative commands cheat sheet To speed up commands, run commands imperatively by using kubectl run instead of creating yaml files. Use help menu A lot of the commands and boilerplate are available through the help menus. For example on ingress: 1 2 3 4 5 6 7 8 9 kubectl create ingress -h # >> Create an ingress with the specified name. # # >> Aliases: # >> ingress, ing # >> Examples: # Create a single ingress called 'simple' that directs requests to foo.com/bar to svc # svc1:8080 with a tls secret \"my-cert\" kubectl create ingress simple --rule = \"foo.com/bar=svc1:8080,tls=my-cert\" Create pods 1 2 3 # Create nginx image # kubectl run <pod name> --image=<image> kubectl run nginx --image = nginx 1 2 3 # Generate POD manifest YAML file without creating the pod (dry-run) # kubectl run <pod name> --image=<image> --dry-run=client -o yaml kubectl run nginx --image = nginx --dry-run = client -o yaml Create deployments 1 2 3 # Create a deployment # kubectl create deployment --image=<image> <name> kubectl create deployment --image = nginx nginx 1 2 3 # Create a deployment YAML file without creating the deployment # kubectl create deployment --image=<image> <name> --dry-run=client -o yaml kubectl create deployment --image = nginx nginx --dry-run = client -o yaml 1 2 3 # Create deployment YAML file with replicas and output to file # kubectl create deployment --image=<image> <name> --replicas=<int> --dry-run=client -o yaml > <name>.yaml kubectl create deployment --image = nginx nginx --replicas = 4 --dry-run = client -o yaml > nginx-deployment.yaml Replace 1 2 # Replace existing file kubectl replace -f <file> --force List all types of resources Listing resource types can be useful to find API versions check if a resource type is bound to namespace or cluster and figuring out the kind and shortnames 1 2 # List resource types kubectl api-resources Create Ingress 1 2 3 kubectl create ingress <ingress-name> --rule = \"host/path=service:port\" kubectl create ingress ingress-test --rule = \"wear.my-online-store.com/wear*=wear-service:80\" Exec in pods Exec without SSHing into the pod 1 kubectl exec -it <pod> -- <commands> Exec with bash 1 kubectl exec --stdin --tty <pod> -- /bin/bash","title":"Certification tips and cheat sheet commands"},{"location":"CKA/CertTips/#certification-tips-and-cheat-sheet-commands","text":"Generally use kubectl describe pod to verify that everything is working as expected.","title":"Certification tips and cheat sheet commands"},{"location":"CKA/CertTips/#setup-by-linux-foundation","text":"At the start of each task you'll be provided with the command to ensure you are on the correct cluster to complete the task. SSH to any node: ssh <nodename> Sudo on any given node: sudo -i You can also use sudo to execute commands with elevated privileges at any time You must return to the base node (hostname node-1) after completing each task. Nested\u2212ssh is not supported. You can use kubectl and the appropriate context to work on any cluster from the base node. When connected to a cluster member via ssh, you will only be able to work on that particular cluster via kubectl. The base system and the cluster nodes have pre-installed and pre-configured: kubectl with k alias and Bash autocompletion jq for YAML/JSON processing tmux for terminal multiplexing curl and wget for testing web services man and man pages for further documentation Further instructions for connecting to cluster nodes will be provided in the appropriate tasks Where no explicit namespace is specified, the default namespace should be acted upon. If you need to destroy/recreate a resource to perform a certain task, it is your responsibility to back up the resource definition appropriately prior to destroying the resource.","title":"Setup by Linux Foundation"},{"location":"CKA/CertTips/#imperative-commands-cheat-sheet","text":"To speed up commands, run commands imperatively by using kubectl run instead of creating yaml files.","title":"Imperative commands cheat sheet"},{"location":"CKA/CertTips/#use-help-menu","text":"A lot of the commands and boilerplate are available through the help menus. For example on ingress: 1 2 3 4 5 6 7 8 9 kubectl create ingress -h # >> Create an ingress with the specified name. # # >> Aliases: # >> ingress, ing # >> Examples: # Create a single ingress called 'simple' that directs requests to foo.com/bar to svc # svc1:8080 with a tls secret \"my-cert\" kubectl create ingress simple --rule = \"foo.com/bar=svc1:8080,tls=my-cert\"","title":"Use help menu"},{"location":"CKA/CertTips/#create-pods","text":"1 2 3 # Create nginx image # kubectl run <pod name> --image=<image> kubectl run nginx --image = nginx 1 2 3 # Generate POD manifest YAML file without creating the pod (dry-run) # kubectl run <pod name> --image=<image> --dry-run=client -o yaml kubectl run nginx --image = nginx --dry-run = client -o yaml","title":"Create pods"},{"location":"CKA/CertTips/#create-deployments","text":"1 2 3 # Create a deployment # kubectl create deployment --image=<image> <name> kubectl create deployment --image = nginx nginx 1 2 3 # Create a deployment YAML file without creating the deployment # kubectl create deployment --image=<image> <name> --dry-run=client -o yaml kubectl create deployment --image = nginx nginx --dry-run = client -o yaml 1 2 3 # Create deployment YAML file with replicas and output to file # kubectl create deployment --image=<image> <name> --replicas=<int> --dry-run=client -o yaml > <name>.yaml kubectl create deployment --image = nginx nginx --replicas = 4 --dry-run = client -o yaml > nginx-deployment.yaml","title":"Create deployments"},{"location":"CKA/CertTips/#replace","text":"1 2 # Replace existing file kubectl replace -f <file> --force","title":"Replace"},{"location":"CKA/CertTips/#list-all-types-of-resources","text":"Listing resource types can be useful to find API versions check if a resource type is bound to namespace or cluster and figuring out the kind and shortnames 1 2 # List resource types kubectl api-resources","title":"List all types of resources"},{"location":"CKA/CertTips/#create-ingress","text":"1 2 3 kubectl create ingress <ingress-name> --rule = \"host/path=service:port\" kubectl create ingress ingress-test --rule = \"wear.my-online-store.com/wear*=wear-service:80\"","title":"Create Ingress"},{"location":"CKA/CertTips/#exec-in-pods","text":"Exec without SSHing into the pod 1 kubectl exec -it <pod> -- <commands> Exec with bash 1 kubectl exec --stdin --tty <pod> -- /bin/bash","title":"Exec in pods"},{"location":"CKA/ClusterRoles/","text":"ClusterRoles and Rolebindings Cluster roles and ClusterRoleBindings are NAMESPACE WIDE! 1 2 3 4 5 6 # Check kubectl api-resources # Print the supported namespaced resources kubectl api-resources --namespaced = true # Print the supported non-namespaced resources kubectl api-resources --namespaced = false As ClusterRoles and RoleBindings are API resources, we can use common verbs on them. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Describe a clusterrole kubectl describe clusterrole cluster-admin # Create a cluster role named \"foo\" with SubResource specified kubectl create clusterrole foo --verb = get,list,watch --resource = pods,pods/status # Describe a clusterrolebinding kubectl describe clusterrolebindings # Create a cluster role named \"foo\" with SubResource specified kubectl create clusterrole foo --verb = get,list,watch --resource = pods,pods/status ####################################################################################################################################### # Create a cluster role named storage-admin with access to storage AND PVs --vv- USE 'kubectl get api-resources' to find resource names kubectl create clusterrole storage-admin --verb = get,list,watch --resource = persistentvolumes,storageclasses # Create a cluster role binding that attaches to the new clusterrole with a user, Michelle. kubectl create clusterrolebinding michelle-storage-admin --user = michelle --clusterrole = storage-admin","title":"ClusterRoles and Rolebindings"},{"location":"CKA/ClusterRoles/#clusterroles-and-rolebindings","text":"Cluster roles and ClusterRoleBindings are NAMESPACE WIDE! 1 2 3 4 5 6 # Check kubectl api-resources # Print the supported namespaced resources kubectl api-resources --namespaced = true # Print the supported non-namespaced resources kubectl api-resources --namespaced = false As ClusterRoles and RoleBindings are API resources, we can use common verbs on them. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Describe a clusterrole kubectl describe clusterrole cluster-admin # Create a cluster role named \"foo\" with SubResource specified kubectl create clusterrole foo --verb = get,list,watch --resource = pods,pods/status # Describe a clusterrolebinding kubectl describe clusterrolebindings # Create a cluster role named \"foo\" with SubResource specified kubectl create clusterrole foo --verb = get,list,watch --resource = pods,pods/status ####################################################################################################################################### # Create a cluster role named storage-admin with access to storage AND PVs --vv- USE 'kubectl get api-resources' to find resource names kubectl create clusterrole storage-admin --verb = get,list,watch --resource = persistentvolumes,storageclasses # Create a cluster role binding that attaches to the new clusterrole with a user, Michelle. kubectl create clusterrolebinding michelle-storage-admin --user = michelle --clusterrole = storage-admin","title":"ClusterRoles and Rolebindings"},{"location":"CKA/Kubectx-kubens/","text":"Kubectx & Kubens CLI's Kubectx 1 2 sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx Syntax for Kubectx 1 2 3 4 5 6 7 8 9 10 11 # List all contexts kubectx # Switch to a new context kubectx <context> # Switch back to previous context kubectx - # Get current context kubectx -c Kubens 1 2 sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens Syntax for Kubens 1 2 3 4 5 6 7 8 # List namespaces kubens # Switch to different namespace kubens <namespace> # Switch back to previous namespace kubens -","title":"Kubectx & Kubens CLI's"},{"location":"CKA/Kubectx-kubens/#kubectx-kubens-clis","text":"","title":"Kubectx &amp; Kubens CLI's"},{"location":"CKA/Kubectx-kubens/#kubectx","text":"1 2 sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx","title":"Kubectx"},{"location":"CKA/Kubectx-kubens/#syntax-for-kubectx","text":"1 2 3 4 5 6 7 8 9 10 11 # List all contexts kubectx # Switch to a new context kubectx <context> # Switch back to previous context kubectx - # Get current context kubectx -c","title":"Syntax for Kubectx"},{"location":"CKA/Kubectx-kubens/#kubens","text":"1 2 sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens","title":"Kubens"},{"location":"CKA/Kubectx-kubens/#syntax-for-kubens","text":"1 2 3 4 5 6 7 8 # List namespaces kubens # Switch to different namespace kubens <namespace> # Switch back to previous namespace kubens -","title":"Syntax for Kubens"},{"location":"CKA/MockExam01/","text":"Answers Q1 1 kubectl -n admin2406 get deployment -o custom-columns = DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:.spec.template.spec.containers [] .image,READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace --sort-by = .metadata.name > /opt/admin2406_data Q2 1 2 kubectl create deployment nginx-deploy --image = nginx:1.16 kubectl set image deployment/nginx-deploy nginx = nginx:1.17 --record Q3 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-alpha-pvc namespace: alpha spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: slow Q4 1 kubectl run secret-1401 -n admin1401 --image = busybox --dry-run = client -o yaml --command -- sleep 4800 > admin.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: secret-1401 name: secret-1401 namespace: admin1401 spec: volumes: - name: secret-volume # secret volume secret: secretName: dotfile-secret containers: - command: - sleep - \"4800\" image: busybox name: secret-admin # volumes' mount path volumeMounts: - name: secret-volume readOnly: true mountPath: \"/etc/secret-volume\"","title":"Answers"},{"location":"CKA/MockExam01/#answers","text":"","title":"Answers"},{"location":"CKA/MockExam01/#q1","text":"1 kubectl -n admin2406 get deployment -o custom-columns = DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:.spec.template.spec.containers [] .image,READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace --sort-by = .metadata.name > /opt/admin2406_data","title":"Q1"},{"location":"CKA/MockExam01/#q2","text":"1 2 kubectl create deployment nginx-deploy --image = nginx:1.16 kubectl set image deployment/nginx-deploy nginx = nginx:1.17 --record","title":"Q2"},{"location":"CKA/MockExam01/#q3","text":"1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-alpha-pvc namespace: alpha spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: slow","title":"Q3"},{"location":"CKA/MockExam01/#q4","text":"1 kubectl run secret-1401 -n admin1401 --image = busybox --dry-run = client -o yaml --command -- sleep 4800 > admin.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: secret-1401 name: secret-1401 namespace: admin1401 spec: volumes: - name: secret-volume # secret volume secret: secretName: dotfile-secret containers: - command: - sleep - \"4800\" image: busybox name: secret-admin # volumes' mount path volumeMounts: - name: secret-volume readOnly: true mountPath: \"/etc/secret-volume\"","title":"Q4"},{"location":"CKA/NetworkPolicies/","text":"Network Policies Standard image of network traffic highlighting ingress and egress . By default ALL pods can communicate with each other in a cluster. However, if we by any chance do not want our services to directly communicate with each other- for example, 'the web server MUST not communicate directly with the database'- we can implement a network policy. Network Policy selectors To link a pod with a network policy, specify labels on the pod and matchLabels on the podSelector on the network policy. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # NetworkPolicy that allows INGRESS from API POD on PORT 3306 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: db-policy spec: podSelector: matchLabels: # This is the DB role: db policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: # Allow ingress FROM api pod role: api-pod ports: - protocol: TCP port: 3306 1 2 3 4 5 6 7 8 9 10 # Pod definition apiVersion: v1 kind: Pod metadata: labels: role: api-pod spec: containers: - name: api image: customApiImage:latest Create a network policy to allow traffic from the Internal application only to the payroll-service and db-service . Policy Name: internal-policy Policy Type: Egress Egress Allow: payroll Payroll Port: 8080 Egress Allow: mysql MySQL Port: 3306 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: internal-policy namespace: default spec: podSelector: matchLabels: name: internal policyTypes: - Egress - Ingress ingress: - {} egress: - to: - podSelector: matchLabels: name: mysql ports: - protocol: TCP port: 3306 - to: - podSelector: matchLabels: name: payroll ports: - protocol: TCP port: 8080 - ports: - port: 53 protocol: UDP - port: 53 protocol: TCP Egress traffic has also been allowed to TCP and UDP. This has been added to ensure that the internal DNS resolution works from the internal pod. Remember: The kube-dns service is exposed on port 53. 1 2 3 4 root@controlplane ~ \u279c kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kube-dns ClusterIP 10 .96.0.10 <none> 53 /UDP,53/TCP,9153/TCP 47m","title":"Network Policies"},{"location":"CKA/NetworkPolicies/#network-policies","text":"Standard image of network traffic highlighting ingress and egress . By default ALL pods can communicate with each other in a cluster. However, if we by any chance do not want our services to directly communicate with each other- for example, 'the web server MUST not communicate directly with the database'- we can implement a network policy.","title":"Network Policies"},{"location":"CKA/NetworkPolicies/#network-policy-selectors","text":"To link a pod with a network policy, specify labels on the pod and matchLabels on the podSelector on the network policy. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # NetworkPolicy that allows INGRESS from API POD on PORT 3306 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: db-policy spec: podSelector: matchLabels: # This is the DB role: db policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: # Allow ingress FROM api pod role: api-pod ports: - protocol: TCP port: 3306 1 2 3 4 5 6 7 8 9 10 # Pod definition apiVersion: v1 kind: Pod metadata: labels: role: api-pod spec: containers: - name: api image: customApiImage:latest Create a network policy to allow traffic from the Internal application only to the payroll-service and db-service . Policy Name: internal-policy Policy Type: Egress Egress Allow: payroll Payroll Port: 8080 Egress Allow: mysql MySQL Port: 3306 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: internal-policy namespace: default spec: podSelector: matchLabels: name: internal policyTypes: - Egress - Ingress ingress: - {} egress: - to: - podSelector: matchLabels: name: mysql ports: - protocol: TCP port: 3306 - to: - podSelector: matchLabels: name: payroll ports: - protocol: TCP port: 8080 - ports: - port: 53 protocol: UDP - port: 53 protocol: TCP Egress traffic has also been allowed to TCP and UDP. This has been added to ensure that the internal DNS resolution works from the internal pod. Remember: The kube-dns service is exposed on port 53. 1 2 3 4 root@controlplane ~ \u279c kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kube-dns ClusterIP 10 .96.0.10 <none> 53 /UDP,53/TCP,9153/TCP 47m","title":"Network Policy selectors"},{"location":"CKA/NetworkTroubleshoot/","text":"Network troubleshooting and network plugins Networking plugins Kubernetes uses CNI plugins to setup network. The kubelet is responsible for executing plugins as we mention the following parameters in kubelet configuration. cni-bin-dir : Kubelet probes this directory for plugins on startup network-plugin : The network plugin to use from cni-bin-dir . It must match the name reported by a plugin probed from the plugin directory. Weave Net This is the only plugin mentioned in the kubernetes documentation. Docs on kubeadm HA using Weavenet Installation 1 kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) \" Flannel Note: Flannel does not support kubernetes network policies. Installation 1 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml Calico Installation 1 2 curl https://docs.projectcalico.org/manifests/calico.yaml -O kubectl apply -f calico.yaml In CKA and CKAD exam, you won\u2019t be asked to install the cni plugin. But if asked you will be provided with the exact url to install it. If not, you can install weave net from the documentation https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/ Note: If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order. DNS in K8s Kubernetes uses CoreDNS. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. Memory and Pods In large scale Kubernetes clusters, CoreDNS\u2019s memory usage is predominantly affected by the number of Pods and Services in the cluster. Other factors include the size of the filled DNS answer cache, and the rate of queries received (QPS) per CoreDNS instance. Kubernetes resources for coreDNS are: A service account named coredns, Cluster-roles named coredns and kube-dns Clusterrolebindings named coredns and kube-dns, A deployment named coredns, A configmap named coredns and a Service named kube-dns. While analyzing the coreDNS deployment you can see that the the Corefile plugin consists of important configuration which is defined as a configmap. Port 53 is used for for DNS resolution. 1 2 3 4 5 kubernetes cluster.local in -addr.arpa ip6.arpa { pods insecure fallthrough in -addr.arpa ip6.arpa ttl 30 } This is the backend to k8s for cluster.local and reverse domains. 1 proxy . /etc/resolv.conf Troubleshooting DNS Troubleshooting issues related to coreDNS If you find CoreDNS pods in pending state first check network plugin is installed. coredns pods have CrashLoopBackOff or Error state If CoreDNS pods and the kube-dns service is working fine, check the kube-dns service has valid endpoints. kubectl -n kube-system get ep kube-dns If there are no endpoints for the service, inspect the service and make sure it uses the correct selectors and ports. Kube Proxy kube-proxy is a network proxy that runs on each node in the cluster. kube-proxy maintains network rules on nodes. These network rules allow network communication to the Pods from network sessions inside or outside of the cluster. In a cluster configured with kubeadm, you can find kube-proxy as a daemonset. If you run a kubectl describe ds kube-proxy -n kube-system you can see that the kube-proxy binary runs with following command inside the kube-proxy container. 1 Command: /usr/local/bin/kube-proxy --config = /var/lib/kube-proxy/config.conf --hostname-override = $( NODE_NAME ) Troubleshooting Kube Proxy Check kube-proxy pod in the kube-system namespace is running. Check kube-proxy logs. Check configmap is correctly defined and the config file for running kube-proxy binary is correct. kube-config is defined in the config map. check kube-proxy is running inside the container 1 2 3 4 5 netstat -plan | grep kube-proxy tcp 0 0 0 .0.0.0:30081 0 .0.0.0:* LISTEN 1 /kube-proxy tcp 0 0 127 .0.0.1:10249 0 .0.0.0:* LISTEN 1 /kube-proxy tcp 0 0 172 .17.0.12:33706 172 .17.0.12:6443 ESTABLISHED 1 /kube-proxy tcp6 0 0 :::10256 :::* LISTEN 1 /kube-proxy Debugging references Services Issues DNS troubleshooting","title":"Network troubleshooting and network plugins"},{"location":"CKA/NetworkTroubleshoot/#network-troubleshooting-and-network-plugins","text":"","title":"Network troubleshooting and network plugins"},{"location":"CKA/NetworkTroubleshoot/#networking-plugins","text":"Kubernetes uses CNI plugins to setup network. The kubelet is responsible for executing plugins as we mention the following parameters in kubelet configuration. cni-bin-dir : Kubelet probes this directory for plugins on startup network-plugin : The network plugin to use from cni-bin-dir . It must match the name reported by a plugin probed from the plugin directory.","title":"Networking plugins"},{"location":"CKA/NetworkTroubleshoot/#weave-net","text":"This is the only plugin mentioned in the kubernetes documentation. Docs on kubeadm HA using Weavenet Installation 1 kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= $( kubectl version | base64 | tr -d '\\n' ) \"","title":"Weave Net"},{"location":"CKA/NetworkTroubleshoot/#flannel","text":"Note: Flannel does not support kubernetes network policies. Installation 1 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml","title":"Flannel"},{"location":"CKA/NetworkTroubleshoot/#calico","text":"Installation 1 2 curl https://docs.projectcalico.org/manifests/calico.yaml -O kubectl apply -f calico.yaml In CKA and CKAD exam, you won\u2019t be asked to install the cni plugin. But if asked you will be provided with the exact url to install it. If not, you can install weave net from the documentation https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/ Note: If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order.","title":"Calico"},{"location":"CKA/NetworkTroubleshoot/#dns-in-k8s","text":"Kubernetes uses CoreDNS. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. Memory and Pods In large scale Kubernetes clusters, CoreDNS\u2019s memory usage is predominantly affected by the number of Pods and Services in the cluster. Other factors include the size of the filled DNS answer cache, and the rate of queries received (QPS) per CoreDNS instance. Kubernetes resources for coreDNS are: A service account named coredns, Cluster-roles named coredns and kube-dns Clusterrolebindings named coredns and kube-dns, A deployment named coredns, A configmap named coredns and a Service named kube-dns. While analyzing the coreDNS deployment you can see that the the Corefile plugin consists of important configuration which is defined as a configmap. Port 53 is used for for DNS resolution. 1 2 3 4 5 kubernetes cluster.local in -addr.arpa ip6.arpa { pods insecure fallthrough in -addr.arpa ip6.arpa ttl 30 } This is the backend to k8s for cluster.local and reverse domains. 1 proxy . /etc/resolv.conf","title":"DNS in K8s"},{"location":"CKA/NetworkTroubleshoot/#troubleshooting-dns","text":"Troubleshooting issues related to coreDNS If you find CoreDNS pods in pending state first check network plugin is installed. coredns pods have CrashLoopBackOff or Error state If CoreDNS pods and the kube-dns service is working fine, check the kube-dns service has valid endpoints. kubectl -n kube-system get ep kube-dns If there are no endpoints for the service, inspect the service and make sure it uses the correct selectors and ports.","title":"Troubleshooting DNS"},{"location":"CKA/NetworkTroubleshoot/#kube-proxy","text":"kube-proxy is a network proxy that runs on each node in the cluster. kube-proxy maintains network rules on nodes. These network rules allow network communication to the Pods from network sessions inside or outside of the cluster. In a cluster configured with kubeadm, you can find kube-proxy as a daemonset. If you run a kubectl describe ds kube-proxy -n kube-system you can see that the kube-proxy binary runs with following command inside the kube-proxy container. 1 Command: /usr/local/bin/kube-proxy --config = /var/lib/kube-proxy/config.conf --hostname-override = $( NODE_NAME )","title":"Kube Proxy"},{"location":"CKA/NetworkTroubleshoot/#troubleshooting-kube-proxy","text":"Check kube-proxy pod in the kube-system namespace is running. Check kube-proxy logs. Check configmap is correctly defined and the config file for running kube-proxy binary is correct. kube-config is defined in the config map. check kube-proxy is running inside the container 1 2 3 4 5 netstat -plan | grep kube-proxy tcp 0 0 0 .0.0.0:30081 0 .0.0.0:* LISTEN 1 /kube-proxy tcp 0 0 127 .0.0.1:10249 0 .0.0.0:* LISTEN 1 /kube-proxy tcp 0 0 172 .17.0.12:33706 172 .17.0.12:6443 ESTABLISHED 1 /kube-proxy tcp6 0 0 :::10256 :::* LISTEN 1 /kube-proxy","title":"Troubleshooting Kube Proxy"},{"location":"CKA/NetworkTroubleshoot/#debugging-references","text":"Services Issues DNS troubleshooting","title":"Debugging references"},{"location":"CKA/NetworkingConcepts/","text":"Networking concepts in K8s Switching, Routing and Gateways Container Network Interface (CNI) Most of kubernetes basic networking is the same as linux networking. See networking in linux Networking in Docker See networking in docker Cluster Networking When setting up K8s in a non-managed cluster we need to ensure that the right ports are open. Below are the required ports for control plane and worker nodes. See the docs for additional info Control plane Protocol Direction Port Range Purpose Used By TCP Inbound 6443 Kubernetes API server All TCP Inbound 2379-2380 etcd server client API kube-apiserver, etcd TCP Inbound 10250 Kubelet API Self, Control plane TCP Inbound 10259 kube-scheduler Self TCP Inbound 10257 kube-controller-manager Self Although etcd ports are included in control plane section, you can also host your own etcd cluster externally or on custom ports. Worker node(s) {#node} Protocol Direction Port Range Purpose Used By TCP Inbound 10250 Kubelet API Self, Control plane TCP Inbound 30000-32767 NodePort Services\u2020 All \u2020 Default port range for NodePort Services . All default port numbers can be overridden. When custom ports are used those ports need to be open instead of defaults mentioned here. One common example is API server port that is sometimes switched to 443. Alternatively, the default port is kept as is and API server is put behind a load balancer that listens on 443 and routes the requests to API server on the default port. Handy debugging commands for exam ip link ip addr ip addr add 192.168.1.10/24 dev eth0 ip route ip route 192.168.1.0/24 via 192.168.2.1 cat /proc/sys/net/ipv4/ip_forward arp netstat -plnt route ip r CNI installation in the exam In the CKA exam, for a question that requires deployment of a network addon, unless specifically directed, any of the solutions described in the links below may be used. However, the documentation currently does not contain a direct reference to the exact command to be used to deploy a third party network addon. The links below redirect to third party/ vendor sites or GitHub repositories which cannot be used in the exam. This has been intentionally done to keep the content in the Kubernetes documentation vendor-neutral. https://kubernetes.io/docs/concepts/cluster-administration/addons/ https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model","title":"Networking concepts in K8s"},{"location":"CKA/NetworkingConcepts/#networking-concepts-in-k8s","text":"","title":"Networking concepts in K8s"},{"location":"CKA/NetworkingConcepts/#switching-routing-and-gateways-container-network-interface-cni","text":"Most of kubernetes basic networking is the same as linux networking. See networking in linux","title":"Switching, Routing and Gateways Container Network Interface (CNI)"},{"location":"CKA/NetworkingConcepts/#networking-in-docker","text":"See networking in docker","title":"Networking in Docker"},{"location":"CKA/NetworkingConcepts/#cluster-networking","text":"When setting up K8s in a non-managed cluster we need to ensure that the right ports are open. Below are the required ports for control plane and worker nodes. See the docs for additional info","title":"Cluster Networking"},{"location":"CKA/NetworkingConcepts/#control-plane","text":"Protocol Direction Port Range Purpose Used By TCP Inbound 6443 Kubernetes API server All TCP Inbound 2379-2380 etcd server client API kube-apiserver, etcd TCP Inbound 10250 Kubelet API Self, Control plane TCP Inbound 10259 kube-scheduler Self TCP Inbound 10257 kube-controller-manager Self Although etcd ports are included in control plane section, you can also host your own etcd cluster externally or on custom ports.","title":"Control plane"},{"location":"CKA/NetworkingConcepts/#worker-nodes-node","text":"Protocol Direction Port Range Purpose Used By TCP Inbound 10250 Kubelet API Self, Control plane TCP Inbound 30000-32767 NodePort Services\u2020 All \u2020 Default port range for NodePort Services . All default port numbers can be overridden. When custom ports are used those ports need to be open instead of defaults mentioned here. One common example is API server port that is sometimes switched to 443. Alternatively, the default port is kept as is and API server is put behind a load balancer that listens on 443 and routes the requests to API server on the default port.","title":"Worker node(s) {#node}"},{"location":"CKA/NetworkingConcepts/#handy-debugging-commands-for-exam","text":"ip link ip addr ip addr add 192.168.1.10/24 dev eth0 ip route ip route 192.168.1.0/24 via 192.168.2.1 cat /proc/sys/net/ipv4/ip_forward arp netstat -plnt route ip r","title":"Handy debugging commands for exam"},{"location":"CKA/NetworkingConcepts/#cni-installation-in-the-exam","text":"In the CKA exam, for a question that requires deployment of a network addon, unless specifically directed, any of the solutions described in the links below may be used. However, the documentation currently does not contain a direct reference to the exact command to be used to deploy a third party network addon. The links below redirect to third party/ vendor sites or GitHub repositories which cannot be used in the exam. This has been intentionally done to keep the content in the Kubernetes documentation vendor-neutral. https://kubernetes.io/docs/concepts/cluster-administration/addons/ https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model","title":"CNI installation in the exam"},{"location":"CKA/NodeDebugging/","text":"Node debugging When facing issues with nodes not being ready, the first thing one should do is SSH into the node and check statuses 1 2 3 4 # Journalctl journalctl -u kubelet # Journalctl -f (follow similar to tail to get to end) 1 2 systemctl status kubelet systemctl restart kubelet In practice exams most of the questions were related to minor issues. If the kubelet is down it's important to check: Is the right certificate being used? (/etc/kubernetes/pki) Is the right port being used (6443) (/etc/kubernetes/kubelet.conf) Does the config file look right? (var/lib/kubelet/config.yaml)","title":"Node debugging"},{"location":"CKA/NodeDebugging/#node-debugging","text":"When facing issues with nodes not being ready, the first thing one should do is SSH into the node and check statuses 1 2 3 4 # Journalctl journalctl -u kubelet # Journalctl -f (follow similar to tail to get to end) 1 2 systemctl status kubelet systemctl restart kubelet In practice exams most of the questions were related to minor issues. If the kubelet is down it's important to check: Is the right certificate being used? (/etc/kubernetes/pki) Is the right port being used (6443) (/etc/kubernetes/kubelet.conf) Does the config file look right? (var/lib/kubelet/config.yaml)","title":"Node debugging"},{"location":"CKA/PrivateRegistry/","text":"Pull from a private registry and pull secret Logging in needs to happen via secrets. Exam / imperative method 1 2 3 4 5 kubectl create secret docker-registry regcred \\ --docker-server = <your-registry-server> \\ --docker-username = <your-name> \\ --docker-password = <your-pword> \\ --docker-email = <your-email> Creating a pod that uses the secret and private registry 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Pod metadata: name: private-reg spec: containers: - name: private-reg-container # Private registry image: my-private-registry.io/tooling/awesome-tool:latest # Pull secret from above imagePullSecrets: - name: regcred Docker way 1 2 # The login process creates or updates a config.json file that holds an authorization token. docker login 1 2 # View the token cat ~/.docker/config.json 1 2 3 4 5 6 7 { \"auths\" : { \"https://index.docker.io/v1/\" : { \"auth\" : \"c3R...zE2\" } } } 1 2 3 4 # Create secret based on 'docker login' config.json kubectl create secret generic regcred \\ --from-file = .dockerconfigjson = <path/to/.docker/config.json> \\ --type = kubernetes.io/dockerconfigjson Alternatively to using the imperative way, a yaml file can also be created which can help if more things need to be added such as a namespace: 1 2 3 4 5 6 7 8 apiVersion: v1 kind: Secret metadata: name: myregistrykey namespace: awesomeapps data: .dockerconfigjson: UmVhbGx5IHJlYWxseSByZWVlZWVlZWV... # This is the dockerconfigjson base64 encoded type: kubernetes.io/dockerconfigjson Inspect the output 1 2 3 4 5 6 # yaml kubectl get secret regcred --output = yaml # dockerconfigjson kubectl get secret regcred --output = \"jsonpath={.data.\\.dockerconfigjson}\" | base64 --decode echo \"c3R...zE2\" | base64 --decode","title":"Pull from a private registry and pull secret"},{"location":"CKA/PrivateRegistry/#pull-from-a-private-registry-and-pull-secret","text":"Logging in needs to happen via secrets.","title":"Pull from a private registry and pull secret"},{"location":"CKA/PrivateRegistry/#exam-imperative-method","text":"1 2 3 4 5 kubectl create secret docker-registry regcred \\ --docker-server = <your-registry-server> \\ --docker-username = <your-name> \\ --docker-password = <your-pword> \\ --docker-email = <your-email>","title":"Exam / imperative method"},{"location":"CKA/PrivateRegistry/#creating-a-pod-that-uses-the-secret-and-private-registry","text":"1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Pod metadata: name: private-reg spec: containers: - name: private-reg-container # Private registry image: my-private-registry.io/tooling/awesome-tool:latest # Pull secret from above imagePullSecrets: - name: regcred","title":"Creating a pod that uses the secret and private registry"},{"location":"CKA/PrivateRegistry/#docker-way","text":"1 2 # The login process creates or updates a config.json file that holds an authorization token. docker login 1 2 # View the token cat ~/.docker/config.json 1 2 3 4 5 6 7 { \"auths\" : { \"https://index.docker.io/v1/\" : { \"auth\" : \"c3R...zE2\" } } } 1 2 3 4 # Create secret based on 'docker login' config.json kubectl create secret generic regcred \\ --from-file = .dockerconfigjson = <path/to/.docker/config.json> \\ --type = kubernetes.io/dockerconfigjson Alternatively to using the imperative way, a yaml file can also be created which can help if more things need to be added such as a namespace: 1 2 3 4 5 6 7 8 apiVersion: v1 kind: Secret metadata: name: myregistrykey namespace: awesomeapps data: .dockerconfigjson: UmVhbGx5IHJlYWxseSByZWVlZWVlZWV... # This is the dockerconfigjson base64 encoded type: kubernetes.io/dockerconfigjson","title":"Docker way"},{"location":"CKA/PrivateRegistry/#inspect-the-output","text":"1 2 3 4 5 6 # yaml kubectl get secret regcred --output = yaml # dockerconfigjson kubectl get secret regcred --output = \"jsonpath={.data.\\.dockerconfigjson}\" | base64 --decode echo \"c3R...zE2\" | base64 --decode","title":"Inspect the output"},{"location":"CKA/SecurityContexts/","text":"Security Contexts Container Security We can enable some container security on Docker. 1 2 docker run --user = 1001 ubuntu sleep 3600 docker run -cap-add MAC_ADMIN ubuntu The same can be done on K8s. Container security can be configured on POD or CONTAINER level or both. If both are specified, the CONTAINER level will override the settings on the POD. Security Context on POD To add security context on the POD and container, add a field called securityContext under the spec section. 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : v1 kind : Pod metadata : name : web-pod spec : ####VVVV####### securityContext : runAsUser : 1000 containers : - name : ubuntu image : ubuntu command : [ \"sleep\" , \"3600\" ] To set the same context at the container level, move the whole section under container section. 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : v1 kind : Pod metadata : name : web-pod spec : containers : - name : ubuntu image : ubuntu command : [ \"sleep\" , \"3600\" ] ####VVVV####### securityContext : runAsUser : 1000 We can also add some capabilities ( ONLY on CONTAINERS. Not PODS ) \" With Linux capabilities, you can grant certain privileges to a process without granting all the privileges of the root user. To add or remove Linux capabilities for a Container, include the capabilities field in the securityContext section of the Container manifest. \" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : web-pod spec : containers : - name : ubuntu image : ubuntu command : [ \"sleep\" , \"3600\" ] securityContext : runAsUser : 1000 ####VVVV##### capabilities : add : [ \"MAC_ADMIN\" ] K8s Reference Docs https://kubernetes.io/docs/tasks/configure-pod-container/security-context/","title":"Security Contexts"},{"location":"CKA/SecurityContexts/#security-contexts","text":"","title":"Security Contexts"},{"location":"CKA/SecurityContexts/#container-security","text":"We can enable some container security on Docker. 1 2 docker run --user = 1001 ubuntu sleep 3600 docker run -cap-add MAC_ADMIN ubuntu The same can be done on K8s. Container security can be configured on POD or CONTAINER level or both. If both are specified, the CONTAINER level will override the settings on the POD.","title":"Container Security"},{"location":"CKA/SecurityContexts/#security-context-on-pod","text":"To add security context on the POD and container, add a field called securityContext under the spec section. 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : v1 kind : Pod metadata : name : web-pod spec : ####VVVV####### securityContext : runAsUser : 1000 containers : - name : ubuntu image : ubuntu command : [ \"sleep\" , \"3600\" ] To set the same context at the container level, move the whole section under container section. 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : v1 kind : Pod metadata : name : web-pod spec : containers : - name : ubuntu image : ubuntu command : [ \"sleep\" , \"3600\" ] ####VVVV####### securityContext : runAsUser : 1000 We can also add some capabilities ( ONLY on CONTAINERS. Not PODS ) \" With Linux capabilities, you can grant certain privileges to a process without granting all the privileges of the root user. To add or remove Linux capabilities for a Container, include the capabilities field in the securityContext section of the Container manifest. \" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : web-pod spec : containers : - name : ubuntu image : ubuntu command : [ \"sleep\" , \"3600\" ] securityContext : runAsUser : 1000 ####VVVV##### capabilities : add : [ \"MAC_ADMIN\" ]","title":"Security Context on POD"},{"location":"CKA/SecurityContexts/#k8s-reference-docs","text":"https://kubernetes.io/docs/tasks/configure-pod-container/security-context/","title":"K8s Reference Docs"},{"location":"CKA/ServiceAccounts/","text":"Service Accounts Two types of users exists in K8s. Service accounts and USERS alias for serviceaccount == sa 1 2 # Create service account named dashboard-sa kubectl create sa dashboard-sa 1 2 # Get service account (sa) kubectl get sa 1 2 3 4 5 6 # Describe service account dashboard-sa kubectl describe sa dashboard-sa # The token is created as a secret, so we have to retrieve # the token by getting secrets. kubectl describe secret dashboard-sa-token-kbbdm When a secret is mounted to a pod, we can see the secret mount path by describing the pod 1 2 3 4 5 6 7 8 9 10 # Describe pod kubectl describe pod dashboard-pod Name : dashboard-pod .. .. .. Mounts : /var/run/secrets/kubernetes.io/serviceaccount from dashboard-sa-token-kbbdm .. Executing commands inside the pod, we can see the secrets listed inside directories 1 2 3 4 5 6 7 8 9 # exec into pod and 'ls' /var/run/secrets/kubernetes.io/serviceaccount kubectl exec -it dashboard-pod ls /var/run/secrets/kubernetes.io/serviceaccount >> ca.crt namespace token # We can also cat to get the token value kubectl exec -it dashboard-pod ca /var/run/secrets/kubernetes.io/serviceaccount/token >> eyJhbGci.... 1 2 # Checking a pod which serviceaccount is linked to it kubectl get po dashboard-pod -o yaml | grep serviceaccount","title":"Service Accounts"},{"location":"CKA/ServiceAccounts/#service-accounts","text":"Two types of users exists in K8s. Service accounts and USERS alias for serviceaccount == sa 1 2 # Create service account named dashboard-sa kubectl create sa dashboard-sa 1 2 # Get service account (sa) kubectl get sa 1 2 3 4 5 6 # Describe service account dashboard-sa kubectl describe sa dashboard-sa # The token is created as a secret, so we have to retrieve # the token by getting secrets. kubectl describe secret dashboard-sa-token-kbbdm When a secret is mounted to a pod, we can see the secret mount path by describing the pod 1 2 3 4 5 6 7 8 9 10 # Describe pod kubectl describe pod dashboard-pod Name : dashboard-pod .. .. .. Mounts : /var/run/secrets/kubernetes.io/serviceaccount from dashboard-sa-token-kbbdm .. Executing commands inside the pod, we can see the secrets listed inside directories 1 2 3 4 5 6 7 8 9 # exec into pod and 'ls' /var/run/secrets/kubernetes.io/serviceaccount kubectl exec -it dashboard-pod ls /var/run/secrets/kubernetes.io/serviceaccount >> ca.crt namespace token # We can also cat to get the token value kubectl exec -it dashboard-pod ca /var/run/secrets/kubernetes.io/serviceaccount/token >> eyJhbGci.... 1 2 # Checking a pod which serviceaccount is linked to it kubectl get po dashboard-pod -o yaml | grep serviceaccount","title":"Service Accounts"},{"location":"CKA/Storage/","text":"Storage and storage types Exam / Imperative method 1 kubectl run <pod name> --image = <image> --dry-run = client -o yaml Volumes On disk files in containers are ephemeral, which obviously presents problems when running containers in general. Docker already has the concept of volumes that can be bound to containers, e.g docker run -d --name devtest -v \"$(pwd)\"/target:/app nginx:latest . Pods in K8's can have any number of volumes or volume types used simultaneously. E.g. AzureDisk CSI or a volume dedicated on the node. Yaml example of volume 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: alpine name: alpine command: [\"/bin/sh\", \"c\"] volumeMounts: - mountPath: /opt name: test-volume volumes: - name: test-volume hostPath: # directory location on host path: /data # this field is optional type: Directory Persistent Volumes A Persistent Volume is a cluster-wide pool of storage volumes configured to be used by users deploying application on the cluster. The users can now select storage from this pool using Persistent Volume Claims. 1 2 3 4 5 6 7 8 9 10 11 12 # pv-definition.yaml kind: PersistentVolume apiVersion: v1 metadata: name: pv-vol1 spec: accessModes: [ \"ReadWriteOnce\" ] capacity: storage: 1Gi hostPath: path: /tmp/data Create the persistent volume 1 2 kubectl create -f pv-definition.yaml # persistentvolume/pv-vol1 created Persistent volume claims Persistent volume claim file 1 2 3 4 5 6 7 8 9 10 11 12 # pvc-definition.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: # Always check that access mode is matching PV accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi Persistent volume file 1 2 3 4 5 6 7 8 9 10 11 12 # pv-definition.yaml kind : PersistentVolume apiVersion : v1 metadata : name : pv-vol1 spec : accessModes : [ \"ReadWriteOnce\" ] capacity : storage : 1Gi hostPath : path : /tmp/data Create the persistent volume for PVC 1 2 kubectl create -f pv-definition.yaml # persistentvolume/pv-vol1 created Create the persistent volume claim (PVC) 1 2 kubectl create -f pvc-definition.yaml # persistentvolumeclaim/myclaim created Pod Definition file using PVC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: webapp name: webapp spec: containers: - image: nginx name: webapp volumeMounts: - mountPath: /log name: webapp volumes: - name: webapp persistentVolumeClaim: claimName: pv-vol1 Storage classes To use external storage - i.e. Azure or GCP, we can either make static provisioning or we can use dynamic provisioning. Static requires to have volumes/disks premade before attaching, where as dynamic provisioning can create volumes/disks on-the-fly. Static provisioning A static provisioning would require two steps. 1) Create disk 1 2 # GCP gcloud beta compute disks create --size 1GB --region westeurope pd-disk 2) Create Persistent volume file 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: PersistentVolume metadata: name: pv-vol spec: accessModes: - ReadWriteOnce capacity: storage: 500Mi # type of disk GCP persistent disk in this case gciPersistentDisk: pdName: pd-disk # same name as was used with the CLI fsType: ext4 Dynamic provisioning When using dynamic provisioning, we use StorageClass as kind instead of PV. There is still created a PV, we just don't have to manage it manually anymore. 1 2 3 4 5 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: google-storage provisioner: kubernetes.io/gce-pd To bind it all together, StorageClass , PersistentVolumeClaim and Pod , the above example might look like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # Pod definition apiVersion: v1 kind: Pod metadata: name: random spec: containers: - image: alpine name: alpine command: [\"/bin/sh\", \"-c\"] args: [\"shuf -i 0-100 -n 1\"] volumeMounts: - mountPath: /opt name: data-volume volumes: - name: data-volume persistentVolumeClaim: # Same name as defined below claimName: gcpClaim --- # PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: # Same name as claimed above name: gcpClaim spec: # Always check that access mode is matching PV accessModes: [ \"ReadWriteOnce\" ] # Same name as specified in the storage class storageClassName: google-storage resources: requests: storage: 1Gi --- # Storage Class apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: # Same name as specified in the PVC name: google-storage provisioner: kubernetes.io/gce-pd","title":"Storage and storage types"},{"location":"CKA/Storage/#storage-and-storage-types","text":"","title":"Storage and storage types"},{"location":"CKA/Storage/#exam-imperative-method","text":"1 kubectl run <pod name> --image = <image> --dry-run = client -o yaml","title":"Exam / Imperative method"},{"location":"CKA/Storage/#volumes","text":"On disk files in containers are ephemeral, which obviously presents problems when running containers in general. Docker already has the concept of volumes that can be bound to containers, e.g docker run -d --name devtest -v \"$(pwd)\"/target:/app nginx:latest . Pods in K8's can have any number of volumes or volume types used simultaneously. E.g. AzureDisk CSI or a volume dedicated on the node.","title":"Volumes"},{"location":"CKA/Storage/#yaml-example-of-volume","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: alpine name: alpine command: [\"/bin/sh\", \"c\"] volumeMounts: - mountPath: /opt name: test-volume volumes: - name: test-volume hostPath: # directory location on host path: /data # this field is optional type: Directory","title":"Yaml example of volume"},{"location":"CKA/Storage/#persistent-volumes","text":"A Persistent Volume is a cluster-wide pool of storage volumes configured to be used by users deploying application on the cluster. The users can now select storage from this pool using Persistent Volume Claims. 1 2 3 4 5 6 7 8 9 10 11 12 # pv-definition.yaml kind: PersistentVolume apiVersion: v1 metadata: name: pv-vol1 spec: accessModes: [ \"ReadWriteOnce\" ] capacity: storage: 1Gi hostPath: path: /tmp/data","title":"Persistent Volumes"},{"location":"CKA/Storage/#create-the-persistent-volume","text":"1 2 kubectl create -f pv-definition.yaml # persistentvolume/pv-vol1 created","title":"Create the persistent volume"},{"location":"CKA/Storage/#persistent-volume-claims","text":"","title":"Persistent volume claims"},{"location":"CKA/Storage/#persistent-volume-claim-file","text":"1 2 3 4 5 6 7 8 9 10 11 12 # pvc-definition.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: # Always check that access mode is matching PV accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi","title":"Persistent volume claim file"},{"location":"CKA/Storage/#persistent-volume-file","text":"1 2 3 4 5 6 7 8 9 10 11 12 # pv-definition.yaml kind : PersistentVolume apiVersion : v1 metadata : name : pv-vol1 spec : accessModes : [ \"ReadWriteOnce\" ] capacity : storage : 1Gi hostPath : path : /tmp/data","title":"Persistent volume file"},{"location":"CKA/Storage/#create-the-persistent-volume-for-pvc","text":"1 2 kubectl create -f pv-definition.yaml # persistentvolume/pv-vol1 created","title":"Create the persistent volume for PVC"},{"location":"CKA/Storage/#create-the-persistent-volume-claim-pvc","text":"1 2 kubectl create -f pvc-definition.yaml # persistentvolumeclaim/myclaim created","title":"Create the persistent volume claim (PVC)"},{"location":"CKA/Storage/#pod-definition-file-using-pvc","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: webapp name: webapp spec: containers: - image: nginx name: webapp volumeMounts: - mountPath: /log name: webapp volumes: - name: webapp persistentVolumeClaim: claimName: pv-vol1","title":"Pod Definition file using PVC"},{"location":"CKA/Storage/#storage-classes","text":"To use external storage - i.e. Azure or GCP, we can either make static provisioning or we can use dynamic provisioning. Static requires to have volumes/disks premade before attaching, where as dynamic provisioning can create volumes/disks on-the-fly.","title":"Storage classes"},{"location":"CKA/Storage/#static-provisioning","text":"A static provisioning would require two steps. 1) Create disk 1 2 # GCP gcloud beta compute disks create --size 1GB --region westeurope pd-disk 2) Create Persistent volume file 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: PersistentVolume metadata: name: pv-vol spec: accessModes: - ReadWriteOnce capacity: storage: 500Mi # type of disk GCP persistent disk in this case gciPersistentDisk: pdName: pd-disk # same name as was used with the CLI fsType: ext4","title":"Static provisioning"},{"location":"CKA/Storage/#dynamic-provisioning","text":"When using dynamic provisioning, we use StorageClass as kind instead of PV. There is still created a PV, we just don't have to manage it manually anymore. 1 2 3 4 5 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: google-storage provisioner: kubernetes.io/gce-pd To bind it all together, StorageClass , PersistentVolumeClaim and Pod , the above example might look like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # Pod definition apiVersion: v1 kind: Pod metadata: name: random spec: containers: - image: alpine name: alpine command: [\"/bin/sh\", \"-c\"] args: [\"shuf -i 0-100 -n 1\"] volumeMounts: - mountPath: /opt name: data-volume volumes: - name: data-volume persistentVolumeClaim: # Same name as defined below claimName: gcpClaim --- # PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: # Same name as claimed above name: gcpClaim spec: # Always check that access mode is matching PV accessModes: [ \"ReadWriteOnce\" ] # Same name as specified in the storage class storageClassName: google-storage resources: requests: storage: 1Gi --- # Storage Class apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: # Same name as specified in the PVC name: google-storage provisioner: kubernetes.io/gce-pd","title":"Dynamic provisioning"},{"location":"ansible/introduction/","text":"Ansible 101 Windows Full docs can be found at docs.ansible Requirements In order to establish a correct ansible setup, we need the following: A controller machine to SSH to target machine Target machines If the host and target machines are Windows based the following requirements need to be met: Ansible can manage desktop OSs including Windows 7, 8.1, and 10, and server OSs including Windows Server 2008, 2008 R2, 2012, 2012 R2, 2016, and 2019. Ansible requires PowerShell 3.0 or newer and at least .NET 4.0 to be installed on the Windows host. A WinRM listener should be created and activated. More details for this can be found below. Upgrade PowerShell and .Net Framework 1 2 3 4 5 6 7 8 9 10 $url = \"https://raw.githubusercontent.com/jborean93/ansible-windows/master/scripts/Upgrade-PowerShell.ps1\" $file = \"$env:temp\\Upgrade-PowerShell.ps1\" $username = \"Administrator\" $password = \"Password\" ( New-Object -TypeName System . Net . WebClient ). DownloadFile ( $url , $file ) Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Force # Version can be 3.0, 4.0 or 5.1 & $file -Version 5 . 1 -Username $username -Password $password -Verbose Once completed, you will need to remove auto logon and set the execution policy back to the default of Restricted. You can do this with the following PowerShell commands: 1 2 3 4 5 6 7 # This isn't needed but is a good security practice to complete Set-ExecutionPolicy -ExecutionPolicy Restricted -Force $reg_winlogon_path = \"HKLM:\\Software\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\" Set-ItemProperty -Path $reg_winlogon_path -Name AutoAdminLogon -Value 0 Remove-ItemProperty -Path $reg_winlogon_path -Name DefaultUserName -ErrorAction SilentlyContinue Remove-ItemProperty -Path $reg_winlogon_path -Name DefaultPassword -ErrorAction SilentlyContinue WinRM Setup 1 2 3 4 5 6 7 8 9 $url = \"https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1\" $file = \"$env:temp\\ConfigureRemotingForAnsible.ps1\" ( New-Object -TypeName System . Net . WebClient ). DownloadFile ( $url , $file ) powershell . exe -ExecutionPolicy ByPass -File $file # There are different switches and parameters (like -EnableCredSSP and -ForceNewSSLCert) that can be set # alongside this script. The documentation for these options are located at the top of the script itself. WinRM Listener The WinRM services listens for requests on one or more ports. Each of these ports must have a listener created and configured. To view the current listeners that are running on the WinRM service, run the following command: 1 winrm enumerate winrm / config / Listener Setup WinRM Listener There are three ways to set up a WinRM listener: Using winrm quickconfig for HTTP or winrm quickconfig -transport:https for HTTPS. This is the easiest option to use when running outside of a domain environment and a simple listener is required. Unlike the other options, this process also has the added benefit of opening up the Firewall for the ports required and starts the WinRM service. Using Group Policy Objects. This is the best way to create a listener when the host is a member of a domain because the configuration is done automatically without any user input. For more information on group policy objects, see the Group Policy Objects documentation. Using PowerShell to create the listener with a specific configuration. This can be done by running the following PowerShell commands: 1 2 3 4 5 6 7 8 9 $selector_set = @{ Address = \"*\" Transport = \"HTTPS\" } $value_set = @{ CertificateThumbprint = \"E6CDAA82EEAF2ECE8546E05DB7F3E01AA47D76CE\" } New-WSManInstance -ResourceURI \"winrm/config/Listener\" -SelectorSet $selector_set -ValueSet $value_set","title":"Ansible 101"},{"location":"ansible/introduction/#ansible-101","text":"","title":"Ansible 101"},{"location":"ansible/introduction/#windows","text":"Full docs can be found at docs.ansible","title":"Windows"},{"location":"ansible/introduction/#requirements","text":"In order to establish a correct ansible setup, we need the following: A controller machine to SSH to target machine Target machines If the host and target machines are Windows based the following requirements need to be met: Ansible can manage desktop OSs including Windows 7, 8.1, and 10, and server OSs including Windows Server 2008, 2008 R2, 2012, 2012 R2, 2016, and 2019. Ansible requires PowerShell 3.0 or newer and at least .NET 4.0 to be installed on the Windows host. A WinRM listener should be created and activated. More details for this can be found below.","title":"Requirements"},{"location":"ansible/introduction/#upgrade-powershell-and-net-framework","text":"1 2 3 4 5 6 7 8 9 10 $url = \"https://raw.githubusercontent.com/jborean93/ansible-windows/master/scripts/Upgrade-PowerShell.ps1\" $file = \"$env:temp\\Upgrade-PowerShell.ps1\" $username = \"Administrator\" $password = \"Password\" ( New-Object -TypeName System . Net . WebClient ). DownloadFile ( $url , $file ) Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Force # Version can be 3.0, 4.0 or 5.1 & $file -Version 5 . 1 -Username $username -Password $password -Verbose Once completed, you will need to remove auto logon and set the execution policy back to the default of Restricted. You can do this with the following PowerShell commands: 1 2 3 4 5 6 7 # This isn't needed but is a good security practice to complete Set-ExecutionPolicy -ExecutionPolicy Restricted -Force $reg_winlogon_path = \"HKLM:\\Software\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\" Set-ItemProperty -Path $reg_winlogon_path -Name AutoAdminLogon -Value 0 Remove-ItemProperty -Path $reg_winlogon_path -Name DefaultUserName -ErrorAction SilentlyContinue Remove-ItemProperty -Path $reg_winlogon_path -Name DefaultPassword -ErrorAction SilentlyContinue","title":"Upgrade PowerShell and .Net Framework"},{"location":"ansible/introduction/#winrm-setup","text":"1 2 3 4 5 6 7 8 9 $url = \"https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1\" $file = \"$env:temp\\ConfigureRemotingForAnsible.ps1\" ( New-Object -TypeName System . Net . WebClient ). DownloadFile ( $url , $file ) powershell . exe -ExecutionPolicy ByPass -File $file # There are different switches and parameters (like -EnableCredSSP and -ForceNewSSLCert) that can be set # alongside this script. The documentation for these options are located at the top of the script itself.","title":"WinRM Setup"},{"location":"ansible/introduction/#winrm-listener","text":"The WinRM services listens for requests on one or more ports. Each of these ports must have a listener created and configured. To view the current listeners that are running on the WinRM service, run the following command: 1 winrm enumerate winrm / config / Listener","title":"WinRM Listener"},{"location":"ansible/introduction/#setup-winrm-listener","text":"There are three ways to set up a WinRM listener: Using winrm quickconfig for HTTP or winrm quickconfig -transport:https for HTTPS. This is the easiest option to use when running outside of a domain environment and a simple listener is required. Unlike the other options, this process also has the added benefit of opening up the Firewall for the ports required and starts the WinRM service. Using Group Policy Objects. This is the best way to create a listener when the host is a member of a domain because the configuration is done automatically without any user input. For more information on group policy objects, see the Group Policy Objects documentation. Using PowerShell to create the listener with a specific configuration. This can be done by running the following PowerShell commands: 1 2 3 4 5 6 7 8 9 $selector_set = @{ Address = \"*\" Transport = \"HTTPS\" } $value_set = @{ CertificateThumbprint = \"E6CDAA82EEAF2ECE8546E05DB7F3E01AA47D76CE\" } New-WSManInstance -ResourceURI \"winrm/config/Listener\" -SelectorSet $selector_set -ValueSet $value_set","title":"Setup WinRM Listener"},{"location":"application-code-basics/nodejs/","text":"NodeJS See also NodeJS express app Node JS is a server side development framework that uses Java Script. It's free, open source and Cross Platform Compatible. Installation On CentOS (used in KodeKlouds lectures) 1 2 3 4 5 6 7 8 9 # Get and install package curl -sL https://rpm.nodesource.com/setup_13.x | bash - yum install node # List version node -v # Run file - add.js node add.js NPM (Node Package Manager) Commands 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # version (could be different from nodeJS) npm -v # Node Search File to search for a package npm search file # install for project -- npm installs it in \"node_modules\" --> \"file\" in working directory npm install file . \u2514\u2500node_modules \u2514\u2500file \u251c\u2500\u2500LICENSE \u251c\u2500\u2500README.md \u251c\u2500\u2500package.json # meta data for package, including name, author, git repo etc. \u2514\u2500\u2500lib # code # list all paths NPM is looking at node -e \"console.log(module.paths)\" # install globally available npm install file -g # built-in modules are located at /usr/lib/node_modules/npm/node_modules/ # External modules (such as react) /usr/lib/node_modules","title":"NodeJS"},{"location":"application-code-basics/nodejs/#nodejs","text":"See also NodeJS express app Node JS is a server side development framework that uses Java Script. It's free, open source and Cross Platform Compatible.","title":"NodeJS"},{"location":"application-code-basics/nodejs/#installation","text":"On CentOS (used in KodeKlouds lectures) 1 2 3 4 5 6 7 8 9 # Get and install package curl -sL https://rpm.nodesource.com/setup_13.x | bash - yum install node # List version node -v # Run file - add.js node add.js","title":"Installation"},{"location":"application-code-basics/nodejs/#npm-node-package-manager","text":"Commands 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # version (could be different from nodeJS) npm -v # Node Search File to search for a package npm search file # install for project -- npm installs it in \"node_modules\" --> \"file\" in working directory npm install file . \u2514\u2500node_modules \u2514\u2500file \u251c\u2500\u2500LICENSE \u251c\u2500\u2500README.md \u251c\u2500\u2500package.json # meta data for package, including name, author, git repo etc. \u2514\u2500\u2500lib # code # list all paths NPM is looking at node -e \"console.log(module.paths)\" # install globally available npm install file -g # built-in modules are located at /usr/lib/node_modules/npm/node_modules/ # External modules (such as react) /usr/lib/node_modules","title":"NPM (Node Package Manager)"},{"location":"application-code-basics/c%23/LINQ/","text":"LINQ (Language-Integrated Query) Introduction Instead of looping through a long array to find specific attributes, we can use LINQ. Beginning of file should include the using System.Linq; and often in conjuction with using System.Collections.Generic; The general syntax is as follows 1 2 3 var < variableName > = from < iterator > in < listObject > where < something > select < something > The idea is basically to shorten the amount of code needed to format or filter lists or arrays. Suppose we need to find all names in a list which are longer than 6 characters and return them in uppercase. There may be an obvious approach of looping the list and formatting with conditionals to return a new formatted list but this will take up more space than the LINQ way of doing it. Below is an example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // Object initialization list - uses curly braces List < string > names = new List < string >{ \"Tom\" , \"Jerry\" , \"Roadrunner\" , \"M. Mouse\" , \"Goofy\" }; // Approach using foreach loop, without LINQ // Basic construction - uses ( ) with no values List < string > longLoudNames = new List < string >(); foreach ( string name in names ) { if ( name . Length > 6 ) { string formattedNames = name . ToUpper (); longLoudNames . Add ( formattedNames ) } } // With LINQ var longLoudNames2 = from n in names where n . Length > 6 select n . ToUpper (); It is also possible to use methods on LINQ collections, as shown below 1 var shortNames = names . Where ( n => n . Length < 4 ); Basics and requirements A basic LINQ query, in query syntax, has three parts: The from operator declares a variable to iterate through the sequence. The where operator picks elements from the sequence if they satisfy the given condition. The condition is normally written like the conditional expressions you would find in an if statement. The select operator determines what is returned for each element in the sequence. Keyword Required from true select true where false The following example exclude the where keyword 1 2 var heroTitles = from hero in heroes select $\"HERO: {hero.ToUpper()}; Var keyword Every LINQ query returns either a single value or an object of type IEnumerable<T> . For IEnumerable<T> , operations such as foreach (loops) and Count() works. The var keyword is an implicitly typed variable. The C# compiler determine the actual type for us. 1 2 string [] names = { \"Tiana\" , \"Dwayne\" , \"Helena\" }; var shortNames = names . Where ( n => n . Length < 4 ); In this case shortNames is actually of type IEnumerable<string> , but we don\u2019t need to worry ourselves about that as long as we have var . Example 1 of LINQ with Query and Method syntax 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 using System ; using System.Collections.Generic ; using System.Linq ; namespace LearnLinq { class Program { static void Main () { List < string > heroes = new List < string > { \"D. Va\" , \"Lucio\" , \"Mercy\" , \"Soldier 76\" , \"Pharah\" , \"Reinhardt\" }; // LINQ query syntax var shortHeroes = from h in heroes where h . Length < 8 select h ; foreach ( var h in shortHeroes ) { // outputs: D. Va, Lucio, Mercy, Pharah Console . WriteLine ( h ); } // LINQ method syntax var longHeroes = heroes . Where ( n => n . Length > 8 ); // outputs 2 Console . WriteLine ( longHeroes . Count ()); } } } Example 2 of LINQ with Query and Method syntax 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 using System ; using System.Collections.Generic ; using System.Linq ; namespace LearnLinq { class Program { static void Main () { string [] heroes = { \"D. Va\" , \"Lucio\" , \"Mercy\" , \"Soldier 76\" , \"Pharah\" , \"Reinhardt\" }; // Query syntax var queryResult = from x in heroes where x . Contains ( \"a\" ) select $\"{x} contains an 'a'\" ; // Method chain syntax var methodResult = heroes . Where ( x => x . Contains ( \"a\" )) . Select ( x => $\"{x} contains an 'a'\" ); // Printing... Console . WriteLine ( \"queryResult:\" ); foreach ( string s in queryResult ) { Console . WriteLine ( s ); } Console . WriteLine ( \"\\nmethodResult:\" ); foreach ( string s in methodResult ) { Console . WriteLine ( s ); } } } }","title":"LINQ (Language-Integrated Query)"},{"location":"application-code-basics/c%23/LINQ/#linq-language-integrated-query","text":"","title":"LINQ (Language-Integrated Query)"},{"location":"application-code-basics/c%23/LINQ/#introduction","text":"Instead of looping through a long array to find specific attributes, we can use LINQ. Beginning of file should include the using System.Linq; and often in conjuction with using System.Collections.Generic; The general syntax is as follows 1 2 3 var < variableName > = from < iterator > in < listObject > where < something > select < something > The idea is basically to shorten the amount of code needed to format or filter lists or arrays. Suppose we need to find all names in a list which are longer than 6 characters and return them in uppercase. There may be an obvious approach of looping the list and formatting with conditionals to return a new formatted list but this will take up more space than the LINQ way of doing it. Below is an example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // Object initialization list - uses curly braces List < string > names = new List < string >{ \"Tom\" , \"Jerry\" , \"Roadrunner\" , \"M. Mouse\" , \"Goofy\" }; // Approach using foreach loop, without LINQ // Basic construction - uses ( ) with no values List < string > longLoudNames = new List < string >(); foreach ( string name in names ) { if ( name . Length > 6 ) { string formattedNames = name . ToUpper (); longLoudNames . Add ( formattedNames ) } } // With LINQ var longLoudNames2 = from n in names where n . Length > 6 select n . ToUpper (); It is also possible to use methods on LINQ collections, as shown below 1 var shortNames = names . Where ( n => n . Length < 4 );","title":"Introduction"},{"location":"application-code-basics/c%23/LINQ/#basics-and-requirements","text":"A basic LINQ query, in query syntax, has three parts: The from operator declares a variable to iterate through the sequence. The where operator picks elements from the sequence if they satisfy the given condition. The condition is normally written like the conditional expressions you would find in an if statement. The select operator determines what is returned for each element in the sequence. Keyword Required from true select true where false The following example exclude the where keyword 1 2 var heroTitles = from hero in heroes select $\"HERO: {hero.ToUpper()};","title":"Basics and requirements"},{"location":"application-code-basics/c%23/LINQ/#var-keyword","text":"Every LINQ query returns either a single value or an object of type IEnumerable<T> . For IEnumerable<T> , operations such as foreach (loops) and Count() works. The var keyword is an implicitly typed variable. The C# compiler determine the actual type for us. 1 2 string [] names = { \"Tiana\" , \"Dwayne\" , \"Helena\" }; var shortNames = names . Where ( n => n . Length < 4 ); In this case shortNames is actually of type IEnumerable<string> , but we don\u2019t need to worry ourselves about that as long as we have var .","title":"Var keyword"},{"location":"application-code-basics/c%23/LINQ/#example-1-of-linq-with-query-and-method-syntax","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 using System ; using System.Collections.Generic ; using System.Linq ; namespace LearnLinq { class Program { static void Main () { List < string > heroes = new List < string > { \"D. Va\" , \"Lucio\" , \"Mercy\" , \"Soldier 76\" , \"Pharah\" , \"Reinhardt\" }; // LINQ query syntax var shortHeroes = from h in heroes where h . Length < 8 select h ; foreach ( var h in shortHeroes ) { // outputs: D. Va, Lucio, Mercy, Pharah Console . WriteLine ( h ); } // LINQ method syntax var longHeroes = heroes . Where ( n => n . Length > 8 ); // outputs 2 Console . WriteLine ( longHeroes . Count ()); } } }","title":"Example 1 of LINQ with Query and Method syntax"},{"location":"application-code-basics/c%23/LINQ/#example-2-of-linq-with-query-and-method-syntax","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 using System ; using System.Collections.Generic ; using System.Linq ; namespace LearnLinq { class Program { static void Main () { string [] heroes = { \"D. Va\" , \"Lucio\" , \"Mercy\" , \"Soldier 76\" , \"Pharah\" , \"Reinhardt\" }; // Query syntax var queryResult = from x in heroes where x . Contains ( \"a\" ) select $\"{x} contains an 'a'\" ; // Method chain syntax var methodResult = heroes . Where ( x => x . Contains ( \"a\" )) . Select ( x => $\"{x} contains an 'a'\" ); // Printing... Console . WriteLine ( \"queryResult:\" ); foreach ( string s in queryResult ) { Console . WriteLine ( s ); } Console . WriteLine ( \"\\nmethodResult:\" ); foreach ( string s in methodResult ) { Console . WriteLine ( s ); } } } }","title":"Example 2 of LINQ with Query and Method syntax"},{"location":"application-code-basics/c%23/c%23_basic_concepts/","text":"Basic concepts This file contains the basics of C# that needs to be remembered and heavily repeated Data types Type Description Size (Bytes) .Net type Range int Whole Numbers 4 System.Int32 +/-2,147,483,648 long Whole numbers (bigger range than int) 8 System.Int64 +/-9,223,372,036,854,775,808 float floating-point numbers 4 System.Single +/-3.4x10^38 double Double precision floating numbers 8 System.Double +/-1.7x10^38 decimal Monetary values (extreme precision such as needed for money) 16 System.Decimal 28 significat figures char Single character e.g 'g' 2 System.Char N/A bool Boolean 1 System.Boolean True or False DateTime Moments in time 8 System.DateTime 0:0:00 on 01/01/0001 to 23:59:59 on 12/31/9999 string Sequence of characters e.g. \"Hello\" System.String N/A Declaring and initializing variables Like other languages variables can be declared and initialized in on single line. The basic structure is as follows 1 2 3 4 5 6 7 8 9 // DataType (e.g. string.) < data_type > < variable_name >; < data_type > < variable_name >= value ; // Access Specifiers //Public, Private, Protected, Internal, Protected internal < access_specifier >< data_type > < variable_name >= value ; Declaring a variable is done by stating it's data type and name without a value. Initialization is done by calling the variable and setting a value, WITHOUT declaring the data type 1 2 3 4 5 6 7 8 9 // Declaration string thisIsAString ; int thisIsAnInt ; bool thisIsABool ; // Initialization thisIsAString = \"Hello World!\" ; thisIsAnInt = 8 ; thisIsABool = true ; Access Specifiers Public Access Specifier It allows a class to expose its member variables and member functions to other functions and objects. Private Access Specifier Private access specifier allows a class to hide its member variables and member functions from other functions and objects. Only functions of the same class can access its private members. Protected Access Specifier Protected access specifier allows a child class to access the member variables and member functions of its base class. Internal Access Specifier Internal access specifier allows a class to expose its member variables and member functions to other functions and objects in the current assembly. Protected Internal Access Specifier The protected internal access specifier allows a class to hide its member variables and member functions from other class objects and functions, except a child class within the same application.","title":"Basic concepts"},{"location":"application-code-basics/c%23/c%23_basic_concepts/#basic-concepts","text":"This file contains the basics of C# that needs to be remembered and heavily repeated","title":"Basic concepts"},{"location":"application-code-basics/c%23/c%23_basic_concepts/#data-types","text":"Type Description Size (Bytes) .Net type Range int Whole Numbers 4 System.Int32 +/-2,147,483,648 long Whole numbers (bigger range than int) 8 System.Int64 +/-9,223,372,036,854,775,808 float floating-point numbers 4 System.Single +/-3.4x10^38 double Double precision floating numbers 8 System.Double +/-1.7x10^38 decimal Monetary values (extreme precision such as needed for money) 16 System.Decimal 28 significat figures char Single character e.g 'g' 2 System.Char N/A bool Boolean 1 System.Boolean True or False DateTime Moments in time 8 System.DateTime 0:0:00 on 01/01/0001 to 23:59:59 on 12/31/9999 string Sequence of characters e.g. \"Hello\" System.String N/A","title":"Data types"},{"location":"application-code-basics/c%23/c%23_basic_concepts/#declaring-and-initializing-variables","text":"Like other languages variables can be declared and initialized in on single line. The basic structure is as follows 1 2 3 4 5 6 7 8 9 // DataType (e.g. string.) < data_type > < variable_name >; < data_type > < variable_name >= value ; // Access Specifiers //Public, Private, Protected, Internal, Protected internal < access_specifier >< data_type > < variable_name >= value ; Declaring a variable is done by stating it's data type and name without a value. Initialization is done by calling the variable and setting a value, WITHOUT declaring the data type 1 2 3 4 5 6 7 8 9 // Declaration string thisIsAString ; int thisIsAnInt ; bool thisIsABool ; // Initialization thisIsAString = \"Hello World!\" ; thisIsAnInt = 8 ; thisIsABool = true ;","title":"Declaring and initializing variables"},{"location":"application-code-basics/c%23/c%23_basic_concepts/#access-specifiers","text":"","title":"Access Specifiers"},{"location":"application-code-basics/c%23/c%23_basic_concepts/#public-access-specifier","text":"It allows a class to expose its member variables and member functions to other functions and objects.","title":"Public Access Specifier"},{"location":"application-code-basics/c%23/c%23_basic_concepts/#private-access-specifier","text":"Private access specifier allows a class to hide its member variables and member functions from other functions and objects. Only functions of the same class can access its private members.","title":"Private Access Specifier"},{"location":"application-code-basics/c%23/c%23_basic_concepts/#protected-access-specifier","text":"Protected access specifier allows a child class to access the member variables and member functions of its base class.","title":"Protected Access Specifier"},{"location":"application-code-basics/c%23/c%23_basic_concepts/#internal-access-specifier","text":"Internal access specifier allows a class to expose its member variables and member functions to other functions and objects in the current assembly.","title":"Internal Access Specifier"},{"location":"application-code-basics/c%23/c%23_basic_concepts/#protected-internal-access-specifier","text":"The protected internal access specifier allows a class to hide its member variables and member functions from other class objects and functions, except a child class within the same application.","title":"Protected Internal Access Specifier"},{"location":"application-code-basics/c%23/interfaces/","text":"Interfaces in C# Interfaces are groups of definitions for related functionalities that a (non-abstract) class or struct must implement. Interfaces specify what a Class MUST have. It does not need to specify what a class MUST NOT have. The interface can't specify Constructors Fields All interfaces are typed as follows, starting by convention with a capital I . 1 2 3 4 interface ISomeInterface { } An interface defines what a class must have. As an example designing cars that abides to the highway patrol's requirements. The highway patrol tells us: \u201cEvery automobile on the road must have these properties and methods accessible to us:\u201d speed license plate number number of wheels ability to honk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // IAutomobile.cs (the interface) using System ; namespace LearnInterfaces { interface IAutomobile { string LicensePlate { get ; } // License plate requirement double Speed { get ; } // Speed requirement int Wheels { get ; } // number of wheels requirement void Honk (); // ability to honk requirement } } // Sedan.cs (the Sedan Class) using System ; namespace LearnInterfaces { class Sedan : IAutomobile { public string LicensePlate { get ; } public double Speed { get ; } public int Wheels { get ; } public void Honk () { Console . WriteLine ( \"HONK HONK!\" ); } } } In the interface, the methods and public variables are only declared, not initialized with any values. See the \"honk\" method. It's only declared like void Honk(); and does not contain a body. The body is defined in the Sedan Class as the Class of the Sedan defines the kind of sound the honk should sound like. For more information see Microsoft's docs","title":"Interfaces in C\\#"},{"location":"application-code-basics/c%23/interfaces/#interfaces-in-c","text":"Interfaces are groups of definitions for related functionalities that a (non-abstract) class or struct must implement. Interfaces specify what a Class MUST have. It does not need to specify what a class MUST NOT have. The interface can't specify Constructors Fields All interfaces are typed as follows, starting by convention with a capital I . 1 2 3 4 interface ISomeInterface { } An interface defines what a class must have. As an example designing cars that abides to the highway patrol's requirements. The highway patrol tells us: \u201cEvery automobile on the road must have these properties and methods accessible to us:\u201d speed license plate number number of wheels ability to honk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // IAutomobile.cs (the interface) using System ; namespace LearnInterfaces { interface IAutomobile { string LicensePlate { get ; } // License plate requirement double Speed { get ; } // Speed requirement int Wheels { get ; } // number of wheels requirement void Honk (); // ability to honk requirement } } // Sedan.cs (the Sedan Class) using System ; namespace LearnInterfaces { class Sedan : IAutomobile { public string LicensePlate { get ; } public double Speed { get ; } public int Wheels { get ; } public void Honk () { Console . WriteLine ( \"HONK HONK!\" ); } } } In the interface, the methods and public variables are only declared, not initialized with any values. See the \"honk\" method. It's only declared like void Honk(); and does not contain a body. The body is defined in the Sedan Class as the Class of the Sedan defines the kind of sound the honk should sound like. For more information see Microsoft's docs","title":"Interfaces in C#"},{"location":"docker/docker_commands/","text":"Docker Commands 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Docker run = run a container from an image Docker run -d = -d is detached mode. It is making the container run in the background ( good idea if you don ' t want the terminal to just wait ) Docker run --name someName <image> = --name to name the container Docker ps = list containers Docker ps -a = list containers running and previous Docker stop + container name or ID = stop container Docker rm + container name or ID = remove container Docker images = list images available on the host Docker rmi = Remove images that you do not want to have any more Docker pull = pull image without running the container Docker run <example ubuntu> sleep 5 = Docker is not meant to host an operating system, rather it is meant to host a process. ubuntu is not a process but an operating system. Docker exec <command> = Use exec to execute a command on a container -- for example docker exec distracted_mcclintock cat /etc/hosts Docker run <image>:<version> = :<version> is called a TAG. If no version is specified, Docker will try to pull the latest available version Docker run -i <image> = -i is for running an app interactively e.g. for input by user Docker run -t < image = -t is for terminal without accepting input. Use only for testing Docker run -it <image> = -it is for interactive and terminal for example for input = \"What is your name?\" . With only -i there would be no prompt, only input. Docker run -p 8383 :8080 <image> = -p is port. 8383 is the internal port and 8080 is the container port to let users in through that port Docker inspect <container name> or <container ID> = inspect the information about the container in JSON Docker logs <container name> or <container ID> = list logs Docker attach = attach a container to the console Docker run -e <some env variable> = <value> <container> = -e sets environvariable given that it is available in the application. Use the \"INSPECT\" command to see if environment variables is already available in a container Docker run --cpus = <float> = --cpus ensures that the container does not use more than a given amount of CPU power. <float> is a relative value from .1 to 1 where .5 is 50 % Docker run --memory = <int>m = same as with CPU. However, specify int instad of float. E.g. --memory = 100m is equivalent of limiting the container to use 100 megabytes of memory. Networks By default Docker only creates one internal network but we can create multiple networks inside the environment. 1 2 3 4 5 6 7 8 9 Docker run <image> = default bridge network Docker run <image> --network = none = docker image is run in an isolated environment Docker run <image> --network = host = docker image is run on the host network, thus two images can not run on the same port Docker network create --driver <bridge> --subnet 182 .18.0.0/16 ( or something else ) <name> Docker network ls = list all networks Docker inspect <id or name of container> = inspect network settings of a container Docker network inspect = inspect network settings of a particular network Docker can connect two containers using ip addresses - for example an SQL instance and a web container: 1 <sql_container name>.connect <web-container ip> --- mysql.connect ( 172 .17.03 ) The above method, however, is not ideal as it is not certain that the container will get the same ip every time. The right way to do it is to reference the container name instead as Docker has an embedded DNS server to resolve DNS. The built in DNS server always run on 127.0.0.11. *** EXAMPLE *** docker network create --driver bridge --subnet 182.18.0.1/24 --gateway 182.18.0.1 wp-mysql-network docker run -d -e MYSQL_ROOT_PASSWORD=db_pass123 --name mysql-db --network wp-mysql-network mysql:5.6 docker pull kodekloud/simple-web-app-mysql docker run --network=wp-mysql-network -e DB_Host=mysql-db -e DB_Password=db_pass123 -p 38080:8080 --name webapp --link mysql-db:mysql-db -d kodekloud/simple-webapp-mysql Dockerfile Dockerfiles must always start with: FROM Data can be stored on the host instead of in the container using the -v . It mounts the storage to the host. Read more on: https://docs.docker.com/storage/ docker run -v /opt/data:/var/lib/mysql -d --name mysql-db -e MYSQL_ROOT_PASSWORD=db_pass123 mysql Docker Registry The registry is the central repository where images are stored. By default images are pulled from docker like docker pull but is actually behind the scenes referred to as: image: / --- e.g. nginx/nginx. The first part is the user or account, and the seocnd part is the image or repository. Therefore the whole \"actual\" command would be image: docker.io/nginx/nginx. ^Registry ^usr/acc ^img/repo Other registries also exist such as: Google's registry is at gcr.io/ Private registries on AWS or Azure is created on subscription/account (maybe?). To pull from private registries remember to login: docker login private-registry.io docker run private-registry.io/apps/internal-app Private registry for On-premise uses the Docker registry application, which is also available as a Docker image. It's exposed on port 5000: docker run -d -p 5000:5000 --name registry registry:2 To push an image to the private registry use the tag to push: 1 2 3 4 5 6 docker image tag <my-image> localhost:5000/<my-image> docker push localhost:5000/<my-image> docker pull localhost:5000/<my-image> #OR docker pull <ip>:5000/<my-image> --- docker pull 192 .168.56.100:5000/<my-image>","title":"Docker Commands"},{"location":"docker/docker_commands/#docker-commands","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Docker run = run a container from an image Docker run -d = -d is detached mode. It is making the container run in the background ( good idea if you don ' t want the terminal to just wait ) Docker run --name someName <image> = --name to name the container Docker ps = list containers Docker ps -a = list containers running and previous Docker stop + container name or ID = stop container Docker rm + container name or ID = remove container Docker images = list images available on the host Docker rmi = Remove images that you do not want to have any more Docker pull = pull image without running the container Docker run <example ubuntu> sleep 5 = Docker is not meant to host an operating system, rather it is meant to host a process. ubuntu is not a process but an operating system. Docker exec <command> = Use exec to execute a command on a container -- for example docker exec distracted_mcclintock cat /etc/hosts Docker run <image>:<version> = :<version> is called a TAG. If no version is specified, Docker will try to pull the latest available version Docker run -i <image> = -i is for running an app interactively e.g. for input by user Docker run -t < image = -t is for terminal without accepting input. Use only for testing Docker run -it <image> = -it is for interactive and terminal for example for input = \"What is your name?\" . With only -i there would be no prompt, only input. Docker run -p 8383 :8080 <image> = -p is port. 8383 is the internal port and 8080 is the container port to let users in through that port Docker inspect <container name> or <container ID> = inspect the information about the container in JSON Docker logs <container name> or <container ID> = list logs Docker attach = attach a container to the console Docker run -e <some env variable> = <value> <container> = -e sets environvariable given that it is available in the application. Use the \"INSPECT\" command to see if environment variables is already available in a container Docker run --cpus = <float> = --cpus ensures that the container does not use more than a given amount of CPU power. <float> is a relative value from .1 to 1 where .5 is 50 % Docker run --memory = <int>m = same as with CPU. However, specify int instad of float. E.g. --memory = 100m is equivalent of limiting the container to use 100 megabytes of memory.","title":"Docker Commands"},{"location":"docker/docker_commands/#networks","text":"By default Docker only creates one internal network but we can create multiple networks inside the environment. 1 2 3 4 5 6 7 8 9 Docker run <image> = default bridge network Docker run <image> --network = none = docker image is run in an isolated environment Docker run <image> --network = host = docker image is run on the host network, thus two images can not run on the same port Docker network create --driver <bridge> --subnet 182 .18.0.0/16 ( or something else ) <name> Docker network ls = list all networks Docker inspect <id or name of container> = inspect network settings of a container Docker network inspect = inspect network settings of a particular network Docker can connect two containers using ip addresses - for example an SQL instance and a web container: 1 <sql_container name>.connect <web-container ip> --- mysql.connect ( 172 .17.03 ) The above method, however, is not ideal as it is not certain that the container will get the same ip every time. The right way to do it is to reference the container name instead as Docker has an embedded DNS server to resolve DNS. The built in DNS server always run on 127.0.0.11. *** EXAMPLE *** docker network create --driver bridge --subnet 182.18.0.1/24 --gateway 182.18.0.1 wp-mysql-network docker run -d -e MYSQL_ROOT_PASSWORD=db_pass123 --name mysql-db --network wp-mysql-network mysql:5.6 docker pull kodekloud/simple-web-app-mysql docker run --network=wp-mysql-network -e DB_Host=mysql-db -e DB_Password=db_pass123 -p 38080:8080 --name webapp --link mysql-db:mysql-db -d kodekloud/simple-webapp-mysql Dockerfile Dockerfiles must always start with: FROM Data can be stored on the host instead of in the container using the -v . It mounts the storage to the host. Read more on: https://docs.docker.com/storage/ docker run -v /opt/data:/var/lib/mysql -d --name mysql-db -e MYSQL_ROOT_PASSWORD=db_pass123 mysql Docker Registry The registry is the central repository where images are stored. By default images are pulled from docker like docker pull but is actually behind the scenes referred to as: image: / --- e.g. nginx/nginx. The first part is the user or account, and the seocnd part is the image or repository. Therefore the whole \"actual\" command would be image: docker.io/nginx/nginx. ^Registry ^usr/acc ^img/repo Other registries also exist such as: Google's registry is at gcr.io/ Private registries on AWS or Azure is created on subscription/account (maybe?). To pull from private registries remember to login: docker login private-registry.io docker run private-registry.io/apps/internal-app Private registry for On-premise uses the Docker registry application, which is also available as a Docker image. It's exposed on port 5000: docker run -d -p 5000:5000 --name registry registry:2 To push an image to the private registry use the tag to push: 1 2 3 4 5 6 docker image tag <my-image> localhost:5000/<my-image> docker push localhost:5000/<my-image> docker pull localhost:5000/<my-image> #OR docker pull <ip>:5000/<my-image> --- docker pull 192 .168.56.100:5000/<my-image>","title":"Networks"},{"location":"docker/docker_networking/","text":"Networking in docker Preinstalled with Docker comes 3 networks Bridge none host 1 2 3 4 5 6 7 # View all networks docker network ls # NETWORK ID NAME DRIVER SCOPE # 2a5096f5fa8c bridge bridge local # b22ad148aeec host host local # e52595512190 none null local The bridge network is the default network a container gets attached to if no network is specified. docker run alpine will attach to the bridge network. If we want to attach to any of the other networks, an explicit assignment needs to be made. docker run alpine --network=none docker run alpine --network=host Bridge network Full docs on docker The bridge network is private internal network created by docker on the host. All containers attached to this network usually gets assigned an ip in the range of 172.17.0.0 . The containers in this network can access each other using their internal 172.17.0.0 ip. To access any of the containers from the host we need to port map them, when spinning up the container docker run -p 80:8080 nginx , where 80 is the internal port and 8080 is the external port. On docker it is called bridge, but on the host, it is called Docker0 1 2 3 4 5 6 ip addr # ... # 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc no queue state DOWN group default # link/ether 02:42:fd:c6:89:59 brd ff:ff:ff:ff:ff:ff # inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 # valid_lft forever preferred_lft forever Host network (Only works on Linux) When using the host network, that container\u2019s network stack is not isolated from the Docker host. The container shares the host\u2019s networking namespace, and the container does not get its own IP-address allocated. For instance, if you run a container which binds to port 5000 and you use host networking, the container\u2019s application is available on port 5000 on the host\u2019s IP address. This also means that we cannot have 2 containers with the same port openings, as they're not isolated from the host. None network With the none network the containers do not have any access to the external host or other containers. They basically run in a completely isolated network. The none network is used by Kubernetes as Docker network does not follow the standard of CNI. When Kubernetes spins up a pod with a container in it the command will be something like 1 2 docker run --network = none <image> bridge add <networkNamespaceId> /var/run/netns/<networkNamespaceId> User-defined networks (recommended) Some of the reasons to use user-defined networks include Better isolation \\ All containers without a --network specified are attached to the default bridge network. This can be a risk, as unrelated stacks/services/containers are then able to communicate. Automatic DNS resolution between containers \\ Containers on the default bridge network can only access each other by IP addresses, unless you use the --link option, which is considered legacy. On a user-defined bridge network, containers can resolve each other by name or alias. Further, using IP addresses is inferior to DNS as the IPs are dynamically provisioned. Imagine an application with a web front-end and a database back-end. If you call your containers web and db, the web container can connect to the db container at db, no matter which Docker host the application stack is running on. If you run the same application stack on the default bridge network, you need to manually create links between the containers (using the legacy --link flag). These links need to be created in both directions, so you can see this gets complex with more than two containers which need to communicate. Alternatively, you can manipulate the /etc/hosts files within the containers, but this creates problems that are difficult to debug. Containers can be attached/detached on the fly \\ During a container\u2019s lifetime, you can connect or disconnect it from user-defined networks on the fly. To remove a container from the default bridge network, you need to stop the container and recreate it with different network options. Creating a custom network A custom defined network can be specified by specifying the subnet and network name 1 2 # Arbitrary subnet range docker network create --driver bridge --subnet 182 .18.0.0/24 custom-isolated-network Inspecting networks 1 docker inspect blissful_hopper","title":"Networking in docker"},{"location":"docker/docker_networking/#networking-in-docker","text":"Preinstalled with Docker comes 3 networks Bridge none host 1 2 3 4 5 6 7 # View all networks docker network ls # NETWORK ID NAME DRIVER SCOPE # 2a5096f5fa8c bridge bridge local # b22ad148aeec host host local # e52595512190 none null local The bridge network is the default network a container gets attached to if no network is specified. docker run alpine will attach to the bridge network. If we want to attach to any of the other networks, an explicit assignment needs to be made. docker run alpine --network=none docker run alpine --network=host","title":"Networking in docker"},{"location":"docker/docker_networking/#bridge-network","text":"Full docs on docker The bridge network is private internal network created by docker on the host. All containers attached to this network usually gets assigned an ip in the range of 172.17.0.0 . The containers in this network can access each other using their internal 172.17.0.0 ip. To access any of the containers from the host we need to port map them, when spinning up the container docker run -p 80:8080 nginx , where 80 is the internal port and 8080 is the external port. On docker it is called bridge, but on the host, it is called Docker0 1 2 3 4 5 6 ip addr # ... # 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc no queue state DOWN group default # link/ether 02:42:fd:c6:89:59 brd ff:ff:ff:ff:ff:ff # inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 # valid_lft forever preferred_lft forever","title":"Bridge network"},{"location":"docker/docker_networking/#host-network-only-works-on-linux","text":"When using the host network, that container\u2019s network stack is not isolated from the Docker host. The container shares the host\u2019s networking namespace, and the container does not get its own IP-address allocated. For instance, if you run a container which binds to port 5000 and you use host networking, the container\u2019s application is available on port 5000 on the host\u2019s IP address. This also means that we cannot have 2 containers with the same port openings, as they're not isolated from the host.","title":"Host network (Only works on Linux)"},{"location":"docker/docker_networking/#none-network","text":"With the none network the containers do not have any access to the external host or other containers. They basically run in a completely isolated network. The none network is used by Kubernetes as Docker network does not follow the standard of CNI. When Kubernetes spins up a pod with a container in it the command will be something like 1 2 docker run --network = none <image> bridge add <networkNamespaceId> /var/run/netns/<networkNamespaceId>","title":"None network"},{"location":"docker/docker_networking/#user-defined-networks-recommended","text":"Some of the reasons to use user-defined networks include Better isolation \\ All containers without a --network specified are attached to the default bridge network. This can be a risk, as unrelated stacks/services/containers are then able to communicate. Automatic DNS resolution between containers \\ Containers on the default bridge network can only access each other by IP addresses, unless you use the --link option, which is considered legacy. On a user-defined bridge network, containers can resolve each other by name or alias. Further, using IP addresses is inferior to DNS as the IPs are dynamically provisioned. Imagine an application with a web front-end and a database back-end. If you call your containers web and db, the web container can connect to the db container at db, no matter which Docker host the application stack is running on. If you run the same application stack on the default bridge network, you need to manually create links between the containers (using the legacy --link flag). These links need to be created in both directions, so you can see this gets complex with more than two containers which need to communicate. Alternatively, you can manipulate the /etc/hosts files within the containers, but this creates problems that are difficult to debug. Containers can be attached/detached on the fly \\ During a container\u2019s lifetime, you can connect or disconnect it from user-defined networks on the fly. To remove a container from the default bridge network, you need to stop the container and recreate it with different network options.","title":"User-defined networks (recommended)"},{"location":"docker/docker_networking/#creating-a-custom-network","text":"A custom defined network can be specified by specifying the subnet and network name 1 2 # Arbitrary subnet range docker network create --driver bridge --subnet 182 .18.0.0/24 custom-isolated-network","title":"Creating a custom network"},{"location":"docker/docker_networking/#inspecting-networks","text":"1 docker inspect blissful_hopper","title":"Inspecting networks"},{"location":"linux-and-bash/bash_cheatsheet/","text":"Bash Cheat Sheet 1 ! /bin/bash ECHO COMMAND 1 echo Hello World! VARIABLES 1 2 3 4 5 #Uppercase by convention #Letters, numbers, underscores NAME = \"Bob\" echo \"My name is $NAME \" echo \"My name is ${ NAME } \" USER INPUT 1 2 read -p \"Enter your name: \" NAME echo \"Hello $NAME , nice to meet you!\" SIMPLE IF STATEMENT 1 2 3 4 if [ \" $NAME \" == \"Brad\" ] then echo \"Your name is Brad\" fi IF-ELSE 1 2 3 4 5 6 if [ \" $NAME \" == \"Brad\" ] then echo \"Your name is Brad\" else echo \"Your name is NOT Brad\" fi ELSE-IF (elif) 1 2 3 4 5 6 7 8 9 if [ \" $NAME \" == \"Brad\" ] then echo \"Your name is Brad\" elif [ \" $NAME \" == \"Jack\" ] then echo \"Your name is Jack\" else echo \"Your name is NOT Brad or Jack\" fi COMPARISON 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 NUM1 = 31 NUM2 = 5 if [ \" $NUM1 \" -gt \" $NUM2 \" ] then echo \" $NUM1 is greater than $NUM2 \" else echo \" $NUM1 is less than $NUM2 \" fi val1 -eq val2 Returns true if the values are equal val1 -ne val2 Returns true if the values are not equal val1 -gt val2 Returns true if val1 is greater than val2 val1 -ge val2 Returns true if val1 is greater than or equal to val2 val1 -lt val2 Returns true if val1 is less than val2 val1 -le val2 Returns true if val1 is less than or equal to val2 FILE CONDITIONS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 FILE = \"test.txt\" if [ -e \" $FILE \" ] then echo \" $FILE exists\" else echo \" $FILE does NOT exist\" fi -d file True if the file is a directory -e file True if the file exists ( note that this is not particularly portable, thus -f is generally used ) -f file True if the provided string is a file -g file True if the group id is set on a file -r file True if the file is readable -s file True if the file has a non-zero size -u True if the user id is set on a file -w True if the file is writable -x True if the file is an executable CASE STATEMENT 1 2 3 4 5 6 7 8 9 10 11 12 read -p \"Are you 21 or over? Y/N \" ANSWER case \" $ANSWER \" in [ yY ] | [ yY ][ eE ][ sS ]) echo \"You can have a beer :)\" ;; [ nN ] | [ nN ][ oO ]) echo \"Sorry, no drinking\" ;; * ) echo \"Please enter y/yes or n/no\" ;; esac SIMPLE FOR LOOP 1 2 3 4 5 NAMES = \"Brad Kevin Alice Mark\" for NAME in $NAMES do echo \"Hello $NAME \" done FOR LOOP TO RENAME FILES 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 FILES = $( ls *.txt ) NEW = \"new\" for FILE in $FILES do echo \"Renaming $FILE to new- $FILE \" mv $FILE $NEW - $FILE done WHILE LOOP - READ THROUGH A FILE LINE BY LINE LINE = 1 while read -r CURRENT_LINE do echo \" $LINE : $CURRENT_LINE \" (( LINE++ )) done < \"./new-1.txt\" FUNCTION 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function sayHello () { echo \"Hello World\" } sayHello FUNCTION WITH PARAMS function greet () { echo \"Hello, I am $1 and I am $2 \" } greet \"Brad\" \"36\" CREATE FOLDER AND WRITE TO A FILE mkdir hello touch \"hello/world.txt\" echo \"Hello World\" >> \"hello/world.txt\" echo \"Created hello/world.txt\"","title":"Bash Cheat Sheet"},{"location":"linux-and-bash/bash_cheatsheet/#bash-cheat-sheet","text":"1 ! /bin/bash ECHO COMMAND 1 echo Hello World! VARIABLES 1 2 3 4 5 #Uppercase by convention #Letters, numbers, underscores NAME = \"Bob\" echo \"My name is $NAME \" echo \"My name is ${ NAME } \" USER INPUT 1 2 read -p \"Enter your name: \" NAME echo \"Hello $NAME , nice to meet you!\" SIMPLE IF STATEMENT 1 2 3 4 if [ \" $NAME \" == \"Brad\" ] then echo \"Your name is Brad\" fi IF-ELSE 1 2 3 4 5 6 if [ \" $NAME \" == \"Brad\" ] then echo \"Your name is Brad\" else echo \"Your name is NOT Brad\" fi ELSE-IF (elif) 1 2 3 4 5 6 7 8 9 if [ \" $NAME \" == \"Brad\" ] then echo \"Your name is Brad\" elif [ \" $NAME \" == \"Jack\" ] then echo \"Your name is Jack\" else echo \"Your name is NOT Brad or Jack\" fi COMPARISON 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 NUM1 = 31 NUM2 = 5 if [ \" $NUM1 \" -gt \" $NUM2 \" ] then echo \" $NUM1 is greater than $NUM2 \" else echo \" $NUM1 is less than $NUM2 \" fi val1 -eq val2 Returns true if the values are equal val1 -ne val2 Returns true if the values are not equal val1 -gt val2 Returns true if val1 is greater than val2 val1 -ge val2 Returns true if val1 is greater than or equal to val2 val1 -lt val2 Returns true if val1 is less than val2 val1 -le val2 Returns true if val1 is less than or equal to val2 FILE CONDITIONS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 FILE = \"test.txt\" if [ -e \" $FILE \" ] then echo \" $FILE exists\" else echo \" $FILE does NOT exist\" fi -d file True if the file is a directory -e file True if the file exists ( note that this is not particularly portable, thus -f is generally used ) -f file True if the provided string is a file -g file True if the group id is set on a file -r file True if the file is readable -s file True if the file has a non-zero size -u True if the user id is set on a file -w True if the file is writable -x True if the file is an executable CASE STATEMENT 1 2 3 4 5 6 7 8 9 10 11 12 read -p \"Are you 21 or over? Y/N \" ANSWER case \" $ANSWER \" in [ yY ] | [ yY ][ eE ][ sS ]) echo \"You can have a beer :)\" ;; [ nN ] | [ nN ][ oO ]) echo \"Sorry, no drinking\" ;; * ) echo \"Please enter y/yes or n/no\" ;; esac SIMPLE FOR LOOP 1 2 3 4 5 NAMES = \"Brad Kevin Alice Mark\" for NAME in $NAMES do echo \"Hello $NAME \" done FOR LOOP TO RENAME FILES 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 FILES = $( ls *.txt ) NEW = \"new\" for FILE in $FILES do echo \"Renaming $FILE to new- $FILE \" mv $FILE $NEW - $FILE done WHILE LOOP - READ THROUGH A FILE LINE BY LINE LINE = 1 while read -r CURRENT_LINE do echo \" $LINE : $CURRENT_LINE \" (( LINE++ )) done < \"./new-1.txt\" FUNCTION 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function sayHello () { echo \"Hello World\" } sayHello FUNCTION WITH PARAMS function greet () { echo \"Hello, I am $1 and I am $2 \" } greet \"Brad\" \"36\" CREATE FOLDER AND WRITE TO A FILE mkdir hello touch \"hello/world.txt\" echo \"Hello World\" >> \"hello/world.txt\" echo \"Created hello/world.txt\"","title":"Bash Cheat Sheet"},{"location":"linux-and-bash/linux_basics/","text":"Linux Basics Most basic commands shell type: echo $SHELL directory tree: mkdir -p dir1/dir2/dir3 remove directory tree: rm -r dir1/dir2/dir3 copy tree: cp -r dir1/dir2/dir3 dirA/dirB/dirC find file: find . -name \"foo*\" Vi editor Two modes: command & insert mode Enter file from terminal: vi file.txt Be in Insert mode - type lowercase \"i\" Escape insert mode: esc Move around with: 1 arrow keys or kjhl # k (line up), j (line down), h (curser left), l (curser right) Delete character with: x Delete entire line: dd Copy: yy Paste: p Scroll up: ctrl+u Scroll down: ctrl+d go to cmd prompt: : (colon) write to file: :w (colon+w); optionally specify filename: \":w filename\" Discard: :q Write & quit : :wq Find: / (slash). E.g. finding the word \"of\"... /of Move curser to occurrences: n User accounts logged in user: whoami user id (UID), group id (GID), groups: id switch user (e.g. su cliff): su ssh with a different user : ssh username@192.168.1.2 root user : global admin.. grant user root permissions in /etc/sudoers file Download files curl : curl https://www.domain.com/some-file.txt -O (-O to save to file, else it will just print file on screen) wget : wget https://www.domain.com/some-file.txt -O fileName.txt OS version ls /etc/*release* cat /etc/*release*","title":"Linux Basics"},{"location":"linux-and-bash/linux_basics/#linux-basics","text":"","title":"Linux Basics"},{"location":"linux-and-bash/linux_basics/#most-basic-commands","text":"shell type: echo $SHELL directory tree: mkdir -p dir1/dir2/dir3 remove directory tree: rm -r dir1/dir2/dir3 copy tree: cp -r dir1/dir2/dir3 dirA/dirB/dirC find file: find . -name \"foo*\"","title":"Most basic commands"},{"location":"linux-and-bash/linux_basics/#vi-editor","text":"Two modes: command & insert mode Enter file from terminal: vi file.txt Be in Insert mode - type lowercase \"i\" Escape insert mode: esc Move around with: 1 arrow keys or kjhl # k (line up), j (line down), h (curser left), l (curser right) Delete character with: x Delete entire line: dd Copy: yy Paste: p Scroll up: ctrl+u Scroll down: ctrl+d go to cmd prompt: : (colon) write to file: :w (colon+w); optionally specify filename: \":w filename\" Discard: :q Write & quit : :wq Find: / (slash). E.g. finding the word \"of\"... /of Move curser to occurrences: n","title":"Vi editor"},{"location":"linux-and-bash/linux_basics/#user-accounts","text":"logged in user: whoami user id (UID), group id (GID), groups: id switch user (e.g. su cliff): su ssh with a different user : ssh username@192.168.1.2 root user : global admin.. grant user root permissions in /etc/sudoers file","title":"User accounts"},{"location":"linux-and-bash/linux_basics/#download-files","text":"curl : curl https://www.domain.com/some-file.txt -O (-O to save to file, else it will just print file on screen) wget : wget https://www.domain.com/some-file.txt -O fileName.txt","title":"Download files"},{"location":"linux-and-bash/linux_basics/#os-version","text":"ls /etc/*release* cat /etc/*release*","title":"OS version"},{"location":"linux-and-bash/networking/","text":"Networking Basic networking concepts... Ip Key Commands list and modify interfaces on the host: iplink list assigned ip addresses to the interfaces: ip addr set ip addresses on the interfaces: ip addr add 192.168.1.10/24 dev eth0 (does not persist reboot, see above example for that) view routing table: route or ip route add entries to the routing table: ip route add 192.168.1.0/24 via 192.168.2.1 From labs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 sudo ip addr add 172 .16.238.15/24 dev eth0 sudo ip addr add 172 .16.238.16/24 dev eth0 sudo ip addr add 172 .16.239.15/24 dev eth0 sudo ip addr add 172 .16.239.16/24 dev eth0 # iNet ip addresses (notice 238, and 239) 172 .16.238.10/24 172 .16.239.10/24 # On machines with ip addr 172.16.238.* sudo ip route add 172 .16.239.0/24 via 172 .16.238.10 # On machines with ip addr 172.16.239.* sudo ip route add 172 .16.238.0/24 via 172 .16.239.10 Switching A switch connects devices within the same network. E.g. 192.168. 1 .0 or 192.168. 2 .0 as shown below The router connects the two switches, i.e. 192.168.1.1 & 192.168.2.1. Running the command ip route add {ipaddress} via {ipaddress} connects the two as shown above. This configuration has to be made on all the systems, meaning it's not enough to only configure one-way. 1 2 ip route add 192 .168.2.0/24 via 192 .168.1.1 ip route add 192 .168.1.0/24 via 192 .168.1.1 This only configures the two networks to talk to each other. If we also want to connect to the internet, say for example Google. We'd need to add that to the routing. 1 2 3 ip route add 192 .168.2.0/24 via 192 .168.1.1 ip route add 192 .168.1.0/24 via 192 .168.2.1 ip route add 172 .217.194.0/24 via 192 .168.2.1 But because we do not know ALL the ip addresses of all hosts, we can set up the router to go via a default routing. ip route add default via 192.168.2.1 . It's basically the same as 0.0.0.0 . However, if we both have an internal network as well as \"public\" network then we'd need to set that up. To make ensure that routing works correctly, use ping {ipaddress} . But on linux it has to be specified in a particular folder. 1 2 3 4 5 6 7 8 9 10 cat proc/sys/net/ipv4/ip_forward # Default value is 0 >> 0 # Does not persist across reboots echo 1 > proc/sys/net/ipv4/ip_forward >> 1 # If host is configured to act as a router configure value in /etc/sysctl.conf net.ipv4.ip_forward = #input number DNS (Domain Name System) Association of information with domain names assigned to entities. 1 2 3 4 5 6 7 # Name resolution # Host association can be found in /etc/hosts # For example >> 192 .168.1.1 db >> 172 .168.2.3 dev.rancher However, this approach is cumbersome if there are hundreds or thousands of servers that need to be rerouted. Therefore a central server to solve this issue evolved, the DNS server. Servers then look at the DNS server instead of each hosts own 'hosts' file. DNS configuration can be found at /etc/resolv.conf . Example.. 1 2 cat /etc/resolv.conf >> nameserver 192 .168.1.100 This does not mean that we can't both have entries in the /etc/hosts file AND in the /etc/resolv.conf files. For example, if we have a test server that doesn't need to be resolved for others, then adding it to /etc/hosts would suffice. To prioritize between the two, we can go to /etc/nsswitch.conf . For internal servers, say we have multiple subdomains like Google has; apps.google.com, drive.google.com etc. We can specify in our /etc/resolv.conf file that we should search within a domain, e.g. Google, for a subdomain, e.g. apps. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat /etc/resolv.conf # resolv.conf >> nameserver 192 .168.1.100 # Adding the search, will allow the DNS to search within a domain for a subdomain. >> search google.com # terminal ping apps >> ping apps.google.com { ipAddress } 56 ( 84 ) bytes of data. # Multiple domains can also be searched for # resolv.conf >> nameserver 192 .168.1.100 # Adding the search, will allow the DNS to search within a domain for a subdomain. >> search google.com prod.google.com Record Types Name Type example ip example A ipv4 web-server 192.168.1.1 AAAA ipv6 web-server -- CNAME Map of an alias food.web-server eat.web-server.com, hungry.web-server.com Ping / nslookup / dig Ping may not always be the best way to check whether an ip is resolved. Nslookup can be beneficial because it provides additional information rather than just checking connectivity. Nslookup does not look in the /etc/hosts file 1 2 3 4 5 6 7 8 nslookup www.google.com >> Server: 8 .8.8.8 >> Address: 8 .8.8.8#53 >> Non-authoritative answer: >> Name: google.com >> Address: 142 .250.74.46 If we want even further details we can use dig . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dig www.google.com >> ; <<>> DiG 9 .10.6 <<>> google.com >> ;; global options: +cmd >> ;; Got answer: >> ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 29436 >> ;; flags: qr rd ra ; QUERY: 1 , ANSWER: 1 , AUTHORITY: 0 , ADDITIONAL: 1 >> ;; OPT PSEUDOSECTION: >> ; EDNS: version: 0 , flags: ; udp: 512 >> ;; QUESTION SECTION: >> ; google.com. IN A >> ;; ANSWER SECTION: >> google.com. 217 IN A 216 .58.211.14 >> ;; Query time: 30 msec >> ;; SERVER: 8 .8.8.8#53 ( 8 .8.8.8 ) >> ;; WHEN: Sat Dec 26 09 :47:27 CET 2020 >> ;; MSG SIZE rcvd: 55","title":"Networking"},{"location":"linux-and-bash/networking/#networking","text":"Basic networking concepts...","title":"Networking"},{"location":"linux-and-bash/networking/#ip-key-commands","text":"list and modify interfaces on the host: iplink list assigned ip addresses to the interfaces: ip addr set ip addresses on the interfaces: ip addr add 192.168.1.10/24 dev eth0 (does not persist reboot, see above example for that) view routing table: route or ip route add entries to the routing table: ip route add 192.168.1.0/24 via 192.168.2.1","title":"Ip Key Commands"},{"location":"linux-and-bash/networking/#from-labs","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 sudo ip addr add 172 .16.238.15/24 dev eth0 sudo ip addr add 172 .16.238.16/24 dev eth0 sudo ip addr add 172 .16.239.15/24 dev eth0 sudo ip addr add 172 .16.239.16/24 dev eth0 # iNet ip addresses (notice 238, and 239) 172 .16.238.10/24 172 .16.239.10/24 # On machines with ip addr 172.16.238.* sudo ip route add 172 .16.239.0/24 via 172 .16.238.10 # On machines with ip addr 172.16.239.* sudo ip route add 172 .16.238.0/24 via 172 .16.239.10","title":"From labs"},{"location":"linux-and-bash/networking/#switching","text":"A switch connects devices within the same network. E.g. 192.168. 1 .0 or 192.168. 2 .0 as shown below The router connects the two switches, i.e. 192.168.1.1 & 192.168.2.1. Running the command ip route add {ipaddress} via {ipaddress} connects the two as shown above. This configuration has to be made on all the systems, meaning it's not enough to only configure one-way. 1 2 ip route add 192 .168.2.0/24 via 192 .168.1.1 ip route add 192 .168.1.0/24 via 192 .168.1.1 This only configures the two networks to talk to each other. If we also want to connect to the internet, say for example Google. We'd need to add that to the routing. 1 2 3 ip route add 192 .168.2.0/24 via 192 .168.1.1 ip route add 192 .168.1.0/24 via 192 .168.2.1 ip route add 172 .217.194.0/24 via 192 .168.2.1 But because we do not know ALL the ip addresses of all hosts, we can set up the router to go via a default routing. ip route add default via 192.168.2.1 . It's basically the same as 0.0.0.0 . However, if we both have an internal network as well as \"public\" network then we'd need to set that up. To make ensure that routing works correctly, use ping {ipaddress} . But on linux it has to be specified in a particular folder. 1 2 3 4 5 6 7 8 9 10 cat proc/sys/net/ipv4/ip_forward # Default value is 0 >> 0 # Does not persist across reboots echo 1 > proc/sys/net/ipv4/ip_forward >> 1 # If host is configured to act as a router configure value in /etc/sysctl.conf net.ipv4.ip_forward = #input number","title":"Switching"},{"location":"linux-and-bash/networking/#dns-domain-name-system","text":"Association of information with domain names assigned to entities. 1 2 3 4 5 6 7 # Name resolution # Host association can be found in /etc/hosts # For example >> 192 .168.1.1 db >> 172 .168.2.3 dev.rancher However, this approach is cumbersome if there are hundreds or thousands of servers that need to be rerouted. Therefore a central server to solve this issue evolved, the DNS server. Servers then look at the DNS server instead of each hosts own 'hosts' file. DNS configuration can be found at /etc/resolv.conf . Example.. 1 2 cat /etc/resolv.conf >> nameserver 192 .168.1.100 This does not mean that we can't both have entries in the /etc/hosts file AND in the /etc/resolv.conf files. For example, if we have a test server that doesn't need to be resolved for others, then adding it to /etc/hosts would suffice. To prioritize between the two, we can go to /etc/nsswitch.conf . For internal servers, say we have multiple subdomains like Google has; apps.google.com, drive.google.com etc. We can specify in our /etc/resolv.conf file that we should search within a domain, e.g. Google, for a subdomain, e.g. apps. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat /etc/resolv.conf # resolv.conf >> nameserver 192 .168.1.100 # Adding the search, will allow the DNS to search within a domain for a subdomain. >> search google.com # terminal ping apps >> ping apps.google.com { ipAddress } 56 ( 84 ) bytes of data. # Multiple domains can also be searched for # resolv.conf >> nameserver 192 .168.1.100 # Adding the search, will allow the DNS to search within a domain for a subdomain. >> search google.com prod.google.com","title":"DNS (Domain Name System)"},{"location":"linux-and-bash/networking/#record-types","text":"Name Type example ip example A ipv4 web-server 192.168.1.1 AAAA ipv6 web-server -- CNAME Map of an alias food.web-server eat.web-server.com, hungry.web-server.com","title":"Record Types"},{"location":"linux-and-bash/networking/#ping-nslookup-dig","text":"Ping may not always be the best way to check whether an ip is resolved. Nslookup can be beneficial because it provides additional information rather than just checking connectivity. Nslookup does not look in the /etc/hosts file 1 2 3 4 5 6 7 8 nslookup www.google.com >> Server: 8 .8.8.8 >> Address: 8 .8.8.8#53 >> Non-authoritative answer: >> Name: google.com >> Address: 142 .250.74.46 If we want even further details we can use dig . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dig www.google.com >> ; <<>> DiG 9 .10.6 <<>> google.com >> ;; global options: +cmd >> ;; Got answer: >> ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 29436 >> ;; flags: qr rd ra ; QUERY: 1 , ANSWER: 1 , AUTHORITY: 0 , ADDITIONAL: 1 >> ;; OPT PSEUDOSECTION: >> ; EDNS: version: 0 , flags: ; udp: 512 >> ;; QUESTION SECTION: >> ; google.com. IN A >> ;; ANSWER SECTION: >> google.com. 217 IN A 216 .58.211.14 >> ;; Query time: 30 msec >> ;; SERVER: 8 .8.8.8#53 ( 8 .8.8.8 ) >> ;; WHEN: Sat Dec 26 09 :47:27 CET 2020 >> ;; MSG SIZE rcvd: 55","title":"Ping / nslookup / dig"},{"location":"linux-and-bash/package_management/","text":"Package management RPM (Redhat Package Manager) install telnet (package): rpm -i telnet.rpm uninstall telnet (package): rpm -e telnet.rpm query package: rpm -q telnet.rpm query multiple: rpm -q telnet.rpm openssh-server ... query all: rpm -qa query specific: rpm -qa | grep packageName Package managers does not install dependencies by themselves. For example, Anisble requires Python to be installed. This is why Yum exists.... Yum is an abstraction layer on RPM, so it uses RPM underneath but with additions to install ALL required packages. Yum commands list yum repo: yum repolist list files in: ls /etc/yum.repos.d/ see url where package is gotten from: cat etc/yum/examplePackage.repo list package: yum list packageName remove package: yum remove packageName list all available versions of package: yum --showduplicates list packageName install package: yum install packageName # use flag -y to confirm install specific version: yum install -y packageName-versionNumber remove package: yum remove packageName Services Services in Linux helps configure software to run in the background and make sure that they run all the time automatically when the servers are booted, and in the right order! Services should start in the right order, especially if there are multiple services which are dependant on each other. HTTPD will be used as example of a service name start HTTPD service: systemctl start httpd || service httpd start stop running service: systemctl stop httpd status of service: systemctl status httpd configure HTTPD to start/stop at startup: systemctl enable/disable httpd Example of a python webapp running 1 2 3 4 5 6 7 8 /usr/bin/python3 /opt/code/my_app.py # output: #>>Serving Flask app \"my_app\" (lazy loading) #>>Environment: production #>> * Running on http://127.0.0.1:5000/ # curl to see if the service is running curl http://localhost:5000 Using systemctl we can start/stop/status the service. start app: systemctl start my_app stop app: systemctl stop my app Configuring application to start on boot or restart if crashing systemctl is used to manage systemd services. So we must configure the program as a systemd service. Systemd unit files are most likely located at /etc/systemd/system The file for the program must be named what the program is eventually to be called. For example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ~/etc/systemd/system # directory > my_app.service # file >> [ Service ] # service >>ExecStart = /usr/bin/python3 /opt/code/my_app.py # application # Let systemd know that of the new service configuration systemctl daemon-reload # Start the service systemctl start my_app # Check status systemctl status my_app # Curl to verify it works curl http://localhost:5000 To configure the service to run on boot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ~/etc/systemd/system # directory my_app.service # file >> [ Unit ] # Additional metadata >> Description = Python web application >> [ Service ] # service >>ExecStart = /usr/bin/python3 /opt/code/my_app.py # application # In case of crash set restart to always >>Restart = always # If other services should be run before or after add: >>ExecPre = /opt/code/somescript1.sh >>ExecPost = /opt/code/somescript2.sh >> [ Install ] >>WantedBy = multi-user.target # Configure to run AFTER the multi-user.target is started # Enable for bootup systemctl enable my_app","title":"Package management"},{"location":"linux-and-bash/package_management/#package-management","text":"","title":"Package management"},{"location":"linux-and-bash/package_management/#rpm-redhat-package-manager","text":"install telnet (package): rpm -i telnet.rpm uninstall telnet (package): rpm -e telnet.rpm query package: rpm -q telnet.rpm query multiple: rpm -q telnet.rpm openssh-server ... query all: rpm -qa query specific: rpm -qa | grep packageName Package managers does not install dependencies by themselves. For example, Anisble requires Python to be installed. This is why Yum exists.... Yum is an abstraction layer on RPM, so it uses RPM underneath but with additions to install ALL required packages.","title":"RPM (Redhat Package Manager)"},{"location":"linux-and-bash/package_management/#yum-commands","text":"list yum repo: yum repolist list files in: ls /etc/yum.repos.d/ see url where package is gotten from: cat etc/yum/examplePackage.repo list package: yum list packageName remove package: yum remove packageName list all available versions of package: yum --showduplicates list packageName install package: yum install packageName # use flag -y to confirm install specific version: yum install -y packageName-versionNumber remove package: yum remove packageName","title":"Yum commands"},{"location":"linux-and-bash/package_management/#services","text":"Services in Linux helps configure software to run in the background and make sure that they run all the time automatically when the servers are booted, and in the right order! Services should start in the right order, especially if there are multiple services which are dependant on each other. HTTPD will be used as example of a service name start HTTPD service: systemctl start httpd || service httpd start stop running service: systemctl stop httpd status of service: systemctl status httpd configure HTTPD to start/stop at startup: systemctl enable/disable httpd","title":"Services"},{"location":"linux-and-bash/package_management/#example-of-a-python-webapp-running","text":"1 2 3 4 5 6 7 8 /usr/bin/python3 /opt/code/my_app.py # output: #>>Serving Flask app \"my_app\" (lazy loading) #>>Environment: production #>> * Running on http://127.0.0.1:5000/ # curl to see if the service is running curl http://localhost:5000 Using systemctl we can start/stop/status the service. start app: systemctl start my_app stop app: systemctl stop my app","title":"Example of a python webapp running"},{"location":"linux-and-bash/package_management/#configuring-application-to-start-on-boot-or-restart-if-crashing","text":"systemctl is used to manage systemd services. So we must configure the program as a systemd service. Systemd unit files are most likely located at /etc/systemd/system The file for the program must be named what the program is eventually to be called. For example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ~/etc/systemd/system # directory > my_app.service # file >> [ Service ] # service >>ExecStart = /usr/bin/python3 /opt/code/my_app.py # application # Let systemd know that of the new service configuration systemctl daemon-reload # Start the service systemctl start my_app # Check status systemctl status my_app # Curl to verify it works curl http://localhost:5000 To configure the service to run on boot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ~/etc/systemd/system # directory my_app.service # file >> [ Unit ] # Additional metadata >> Description = Python web application >> [ Service ] # service >>ExecStart = /usr/bin/python3 /opt/code/my_app.py # application # In case of crash set restart to always >>Restart = always # If other services should be run before or after add: >>ExecPre = /opt/code/somescript1.sh >>ExecPost = /opt/code/somescript2.sh >> [ Install ] >>WantedBy = multi-user.target # Configure to run AFTER the multi-user.target is started # Enable for bootup systemctl enable my_app","title":"Configuring application to start on boot or restart if crashing"},{"location":"linux-and-bash/shell_scripts/shell_101/","text":"Introduction to Shell Scripts The following contains the very basics of shell scripts! Shell scripts as commands While shell scripts usually have .sh appended to them, we can export the shell script to the PATH to commands and use the terminal to fire them. When doing this, we do not want the file to end in .sh but rather just be a file with no ending 1 2 #shellscriptfile mkdir somedir 1 2 3 4 5 #terminal export PATH = $PATH :/Documents/shellscriptfile # See where a command is located from which shellscriptfile Adding text to file using echo & cat Create a script named create-and-launch-rocket at the path /home/bob and add the below commands to it. mkdir lunar-mission rocket-add lunar-mission rocket-start-power lunar-mission rocket-internal-power lunar-mission rocket-start-sequence lunar-mission rocket-start-engine lunar-mission rocket-lift-off lunar-mission rocket-status lunar-mission 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #echo \"content\" > file #echo \"addmorecontent\" >> ExistingFile echo \"mkdir lunar-mission\" > create-and-launch-rocket echo \"rocket-add lunar-mission\" >> create-and-launch-rocket # using cat # existing file cat >> /path/to/existingFile.text << EOF some text line 1 some text line 2 some text line 3 EOF # new file cat > /path/to/newFile.text << EOF some text line 1 some text line 2 some text line 3 EOF # Using Vi mission_name = lunar-mission mkdir $mission_name rocket-add $mission_name rocket-start-power $mission_name rocket-internal-power $mission_name rocket-start-sequence $mission_name rocket-start-engine $mission_name rocket-lift-off $mission_name rocket-status $mission_name Create a shell script in the home directory called create-directory-structure.sh. The script should do the following tasks: Create the following directories under /home/bob/countries - USA, UK, India Create a file under each directory by the name capital.txt Add the capital cities name in the file - Washington, D.C, London, New Delhi Print uptime of the system 1 2 3 4 5 6 7 mkdir countries cd countries mkdir USA India UK echo \"Washington, D.C\" > USA/capital.txt echo \"London\" > UK/capital.txt echo \"New Delhi\" > India/capital.txt uptime A script by the name backup-file.sh is placed in your home directory at /home/bob. This script creates a backup of a file by creating a copy of the same file and apending _bkp to it's name. However it's not working'. Please inspect and fix the problem with the script. 1 2 3 4 5 6 7 8 # current contents file_name = \"create-and-launch-rocket\" cp $file_name $file_name_bkp # Use curly braces to fix the issue. file_name = \"create-and-launch-rocket\" cp $file_name ${ file_name } _bkp","title":"Introduction to Shell Scripts"},{"location":"linux-and-bash/shell_scripts/shell_101/#introduction-to-shell-scripts","text":"The following contains the very basics of shell scripts!","title":"Introduction to Shell Scripts"},{"location":"linux-and-bash/shell_scripts/shell_101/#shell-scripts-as-commands","text":"While shell scripts usually have .sh appended to them, we can export the shell script to the PATH to commands and use the terminal to fire them. When doing this, we do not want the file to end in .sh but rather just be a file with no ending 1 2 #shellscriptfile mkdir somedir 1 2 3 4 5 #terminal export PATH = $PATH :/Documents/shellscriptfile # See where a command is located from which shellscriptfile","title":"Shell scripts as commands"},{"location":"linux-and-bash/shell_scripts/shell_101/#adding-text-to-file-using-echo-cat","text":"Create a script named create-and-launch-rocket at the path /home/bob and add the below commands to it. mkdir lunar-mission rocket-add lunar-mission rocket-start-power lunar-mission rocket-internal-power lunar-mission rocket-start-sequence lunar-mission rocket-start-engine lunar-mission rocket-lift-off lunar-mission rocket-status lunar-mission 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #echo \"content\" > file #echo \"addmorecontent\" >> ExistingFile echo \"mkdir lunar-mission\" > create-and-launch-rocket echo \"rocket-add lunar-mission\" >> create-and-launch-rocket # using cat # existing file cat >> /path/to/existingFile.text << EOF some text line 1 some text line 2 some text line 3 EOF # new file cat > /path/to/newFile.text << EOF some text line 1 some text line 2 some text line 3 EOF # Using Vi mission_name = lunar-mission mkdir $mission_name rocket-add $mission_name rocket-start-power $mission_name rocket-internal-power $mission_name rocket-start-sequence $mission_name rocket-start-engine $mission_name rocket-lift-off $mission_name rocket-status $mission_name Create a shell script in the home directory called create-directory-structure.sh. The script should do the following tasks: Create the following directories under /home/bob/countries - USA, UK, India Create a file under each directory by the name capital.txt Add the capital cities name in the file - Washington, D.C, London, New Delhi Print uptime of the system 1 2 3 4 5 6 7 mkdir countries cd countries mkdir USA India UK echo \"Washington, D.C\" > USA/capital.txt echo \"London\" > UK/capital.txt echo \"New Delhi\" > India/capital.txt uptime A script by the name backup-file.sh is placed in your home directory at /home/bob. This script creates a backup of a file by creating a copy of the same file and apending _bkp to it's name. However it's not working'. Please inspect and fix the problem with the script. 1 2 3 4 5 6 7 8 # current contents file_name = \"create-and-launch-rocket\" cp $file_name $file_name_bkp # Use curly braces to fix the issue. file_name = \"create-and-launch-rocket\" cp $file_name ${ file_name } _bkp","title":"Adding text to file using echo &amp; cat"},{"location":"webservers/Apache-tomcat/","text":"Apache Tomcat Apache Tomcat provides a webserver to host Java based web applications. A prerequisite on the server is to have Java installed. Java Installation 1 yum install java-1.8.0-openjdk-devel Apache Tomcat Installation The Apache Tomcat also has to be downloaded from the apache website . Afterwards extract the downloaded package. 1 2 3 4 5 6 7 8 wget https://downloads.apache.org/tomcat/tomcat-8/v8.5.53/bin/apache-tomcat-8.5.53.tar.gz tar xvf apache-tomcat-8.5.53.tar.gz # start the server ./apache-tomcat-8.5.53/bin/startup.sh # check if it works https://localhost:8080 Tomcat directory Files stored in the Tomcat directory is apache-tomcat-8.5.53 . 1 ls -l apache-tomcat-8.5.53 Bin directory In a subdirectory, the bin directory is located. It contains .bat files for Windows and .sh files for Linux based systems. Furthermore, the startup scripts startup.sh and shutdown scripts shutdown.sh are also located there. Conf directory Another subdirectory contains the configuration files in the conf directory. This is where the webserver is configured, e.g. what port to listen on or how to direct traffic between web apps. Webapps directory The webapps directory contains the webapps hosted by Tomcat. This is the place where the we want Apache to serve. Deploying the application to Apache Tomcat Since Apache is used for Java based applications, we need to package the application. Multiple approaches can be used for this task. Java native Maven Gradle 1 2 // .war = Web Archive jar - cvf app . war * 1 mvn package 1 maven build Once the application is packaged, move the .war file to the webapps directory. If the service is running, it will automatically unpackage the .war file. However, check whether it's running may be a good idea. 1 2 3 4 5 # View it in the log cat ~/apache-tomcat-8.5.53/logs/catalina.out # View in browser https://localhost:8080/ { AppName } From the labs Install latest version of tomcat 8 on host01 server under /opt/ directory. Please move extracted package to /opt/apache-tomcat-8 to keep just major version for simplicity. Download 1 2 3 sudo curl -O https://downloads.apache.org/tomcat/tomcat-8/v8.5.61/bin/apache-tomcat-8.5.61.tar.gz sudo tar -xvf apache-tomcat-8.5.61.tar.gz sudo mv apache-tomcat-8.5.61 /opt/apache-tomcat-8 Start & Check if running Start service and check if tomcat is running on port 8080 1 2 3 4 5 # Start service /apache-tomcat-8/bin/startup.sh # Check localhost if running curl https://localhost:8080 ; ps -ef | grep tomcat Check if running on port 9090 1 2 3 4 sudo sed -i 's/8080/9090/g' /opt/apache-tomcat-8/conf/server.xml sudo /opt/apache-tomcat-8/bin/shutdown.sh sudo /opt/apache-tomcat-8/bin/startup.sh curl localhost:9090 ; ps -ef | grep tomcat","title":"Apache Tomcat"},{"location":"webservers/Apache-tomcat/#apache-tomcat","text":"Apache Tomcat provides a webserver to host Java based web applications. A prerequisite on the server is to have Java installed.","title":"Apache Tomcat"},{"location":"webservers/Apache-tomcat/#java-installation","text":"1 yum install java-1.8.0-openjdk-devel","title":"Java Installation"},{"location":"webservers/Apache-tomcat/#apache-tomcat-installation","text":"The Apache Tomcat also has to be downloaded from the apache website . Afterwards extract the downloaded package. 1 2 3 4 5 6 7 8 wget https://downloads.apache.org/tomcat/tomcat-8/v8.5.53/bin/apache-tomcat-8.5.53.tar.gz tar xvf apache-tomcat-8.5.53.tar.gz # start the server ./apache-tomcat-8.5.53/bin/startup.sh # check if it works https://localhost:8080","title":"Apache Tomcat Installation"},{"location":"webservers/Apache-tomcat/#tomcat-directory","text":"Files stored in the Tomcat directory is apache-tomcat-8.5.53 . 1 ls -l apache-tomcat-8.5.53","title":"Tomcat directory"},{"location":"webservers/Apache-tomcat/#bin-directory","text":"In a subdirectory, the bin directory is located. It contains .bat files for Windows and .sh files for Linux based systems. Furthermore, the startup scripts startup.sh and shutdown scripts shutdown.sh are also located there.","title":"Bin directory"},{"location":"webservers/Apache-tomcat/#conf-directory","text":"Another subdirectory contains the configuration files in the conf directory. This is where the webserver is configured, e.g. what port to listen on or how to direct traffic between web apps.","title":"Conf directory"},{"location":"webservers/Apache-tomcat/#webapps-directory","text":"The webapps directory contains the webapps hosted by Tomcat. This is the place where the we want Apache to serve.","title":"Webapps directory"},{"location":"webservers/Apache-tomcat/#deploying-the-application-to-apache-tomcat","text":"Since Apache is used for Java based applications, we need to package the application. Multiple approaches can be used for this task. Java native Maven Gradle 1 2 // .war = Web Archive jar - cvf app . war * 1 mvn package 1 maven build Once the application is packaged, move the .war file to the webapps directory. If the service is running, it will automatically unpackage the .war file. However, check whether it's running may be a good idea. 1 2 3 4 5 # View it in the log cat ~/apache-tomcat-8.5.53/logs/catalina.out # View in browser https://localhost:8080/ { AppName }","title":"Deploying the application to Apache Tomcat"},{"location":"webservers/Apache-tomcat/#from-the-labs","text":"Install latest version of tomcat 8 on host01 server under /opt/ directory. Please move extracted package to /opt/apache-tomcat-8 to keep just major version for simplicity.","title":"From the labs"},{"location":"webservers/Apache-tomcat/#download","text":"1 2 3 sudo curl -O https://downloads.apache.org/tomcat/tomcat-8/v8.5.61/bin/apache-tomcat-8.5.61.tar.gz sudo tar -xvf apache-tomcat-8.5.61.tar.gz sudo mv apache-tomcat-8.5.61 /opt/apache-tomcat-8","title":"Download"},{"location":"webservers/Apache-tomcat/#start-check-if-running","text":"Start service and check if tomcat is running on port 8080 1 2 3 4 5 # Start service /apache-tomcat-8/bin/startup.sh # Check localhost if running curl https://localhost:8080 ; ps -ef | grep tomcat","title":"Start &amp; Check if running"},{"location":"webservers/Apache-tomcat/#check-if-running-on-port-9090","text":"1 2 3 4 sudo sed -i 's/8080/9090/g' /opt/apache-tomcat-8/conf/server.xml sudo /opt/apache-tomcat-8/bin/shutdown.sh sudo /opt/apache-tomcat-8/bin/startup.sh curl localhost:9090 ; ps -ef | grep tomcat","title":"Check if running on port 9090"},{"location":"webservers/Apache-web-server/","text":"Apache Web Server Open source http server. It's a webserver usually used to serve HTML, CSS & JS. However, it is also often used in conjunction with an application server. Webservers usually serves static sites whereas application servers serve dynamic sites, to process logic and transactions. In CentOS apache is already included. In CentOS Apache is already available as a web server. 1 2 3 4 5 6 7 8 9 10 11 # Install Apache Web Server yum install httpd # Start Service service httpd start # Check status service httpd status # Allow http (port 80) traffic firewall-cmd --permanent --add-service = http Logs Logs are located at the following paths 1 2 3 4 5 # Access Log - whenever a user accesses the website /var/log/httpd/access_log # Error Log - whenvever an error occurs /var/log/httpd/error_log Config File Look for configurations such as ServerName, DocumentRoot, Listen (port) etc. Important to note ANY changes in the config file requires service restart service httpd restart 1 /etc/httpd/conf/httpd.conf To point ServerName (e.g. www.houses.com:80), we have to point it to a DNS entry in the DNS server. Otherwise it won't know where to look for. Can also be done in the /etc/hosts to look in the local system first. 1 2 3 4 # /etc/hosts 127 .0.0.1 localhost 127 .0.0.1 www.houses.com An apache webserver can host multiple sites. This is also specified in the config file. When having multiple sites, it also makes sense to move each config file to their respective directory to not mix the two together. 1 2 3 4 5 6 7 8 9 10 # houses.com /etc/httpd/conf/houses.conf # oranges.com /etc/httpd/conf/oranges.conf # In the original config file include the new paths # config file Include conf/houses.conf Include conf/oranges.conf Access site 1 2 3 4 http://localhost:80 # Since it's http, port number is not needed. http://localhost","title":"Apache Web Server"},{"location":"webservers/Apache-web-server/#apache-web-server","text":"Open source http server. It's a webserver usually used to serve HTML, CSS & JS. However, it is also often used in conjunction with an application server. Webservers usually serves static sites whereas application servers serve dynamic sites, to process logic and transactions. In CentOS apache is already included. In CentOS Apache is already available as a web server. 1 2 3 4 5 6 7 8 9 10 11 # Install Apache Web Server yum install httpd # Start Service service httpd start # Check status service httpd status # Allow http (port 80) traffic firewall-cmd --permanent --add-service = http","title":"Apache Web Server"},{"location":"webservers/Apache-web-server/#logs","text":"Logs are located at the following paths 1 2 3 4 5 # Access Log - whenever a user accesses the website /var/log/httpd/access_log # Error Log - whenvever an error occurs /var/log/httpd/error_log","title":"Logs"},{"location":"webservers/Apache-web-server/#config-file","text":"Look for configurations such as ServerName, DocumentRoot, Listen (port) etc. Important to note ANY changes in the config file requires service restart service httpd restart 1 /etc/httpd/conf/httpd.conf To point ServerName (e.g. www.houses.com:80), we have to point it to a DNS entry in the DNS server. Otherwise it won't know where to look for. Can also be done in the /etc/hosts to look in the local system first. 1 2 3 4 # /etc/hosts 127 .0.0.1 localhost 127 .0.0.1 www.houses.com An apache webserver can host multiple sites. This is also specified in the config file. When having multiple sites, it also makes sense to move each config file to their respective directory to not mix the two together. 1 2 3 4 5 6 7 8 9 10 # houses.com /etc/httpd/conf/houses.conf # oranges.com /etc/httpd/conf/oranges.conf # In the original config file include the new paths # config file Include conf/houses.conf Include conf/oranges.conf","title":"Config File"},{"location":"webservers/Apache-web-server/#access-site","text":"1 2 3 4 http://localhost:80 # Since it's http, port number is not needed. http://localhost","title":"Access site"},{"location":"webservers/IPs_and_ports/","text":"Ip addresses and Ports from a Web Applciation Perspective FAQ What IP address and port should be used? localhost vs 127.0.0.1 vs IP address? Why can't I connect to my server? Ip Address and ports basics Laptops, servers etc. have different kinds of interfaces and adapters for connectivity such as Wired ethernet intefaces to connect to a LAN through a hub or switch, or Wireless to connect through Wi-Fi. No matter how the device is connected, it will get an ip address assigned to the device. A device can have multiple ip addresses if they are connected to more than one adapter. For example, a laptop is both connected through ethernet and a wireless connection - illustrated below. Each of the Network Interface Cards (NIC) are divided into logical components known as ports. It's possible to have up to 65535 ports on each ip address. Each port is a communication endpoint. Programs can listen on these ports for requests. For example, a flask application listens on port 5000 by default, but can be configured in the code. If the device has multiple ip addresses the configuration for listening on the correct one also has to be configured. The following example illustrates the point. 1 2 3 4 5 6 7 8 9 10 11 # main.py from flask import Flask app = Flask ( __name__ ) @app . route ( '/' ) def hello (): return 'Hello World!' if __name__ == '__main__' : app . run ( port = 8000 , host = 10.0.2.15 ) The example makes the application available to a single ip address but it is also possible for both ip addresses ( 10.0.2.15 and 10.0.2.16 ) to be available for users to connect to. Instead we can specify the host to listen on all available interfaces with host=0.0.0.0 . Now the application is available to everyone to see, but if we want to just develop locally, we can simply remove the host= . When no option is specified the application will automatically listen on 127.0.0.1 (the loopback address). The loopback address is the same on all devices, but the name localhost is the same. So we can access our test site using either http://localhost:8000 or http://127.0.0.1:8000 . Labs Exercise 1 Run apache tomcat on all IP interfaces and port 9090 on host01 server. We already downloaded apache tomcat under /opt/apache-tomcat-8.5.53 directory. Modify the necessary files and start tomcat. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 cd /opt/apache-tomcat-8.5.53 cd conf > permission denied sudo chmod -R 757 /opt cd conf cat server.xml ########## Output Snippet ########## > A \"Connector\" represents an endpoint by which requests are received > and responses are returned. Documentation at : > Java HTTP Connector: /docs/config/http.html > Java AJP Connector: /docs/config/ajp.html > APR ( HTTP/AJP ) Connector: /docs/apr.html > Define a non-SSL/TLS HTTP/1.1 Connector on port 8080 > <Connector port = \"8080\" protocol = \"HTTP/1.1\" > connectionTimeout = \"20000\" > redirectPort = \"8443\" /> ########## End Snippet ########## sudo sed -i 's/8080/9090/g' apache-tomcat-8.5.53/conf/server.xml ########## Output Snippet ########## > A \"Connector\" represents an endpoint by which requests are received > and responses are returned. Documentation at : > Java HTTP Connector: /docs/config/http.html > Java AJP Connector: /docs/config/ajp.html > APR ( HTTP/AJP ) Connector: /docs/apr.html > Define a non-SSL/TLS HTTP/1.1 Connector on port 9090 > <Connector port = \"9090\" protocol = \"HTTP/1.1\" > connectionTimeout = \"20000\" > redirectPort = \"8443\" /> > A \"Connector\" using the shared thread pool > <Connector executor = \"tomcatThreadPool\" > port = \"9090\" protocol = \"HTTP/1.1\" > connectionTimeout = \"20000\" > redirectPort = \"8443\" /> ########## End Snippet ########## sudo ./apache-tomcat-8.5.53/bin/startup.sh curl localhost:9090 ; ps -ef | grep tomcat Exercise 2 We have cloned python app repo from github in /opt/ directory in app01 server. When you start it which IP and port combination it will listen on? Python app file : /opt/simple-webapp-flask/app.py 1 2 3 4 5 6 7 8 9 10 11 # Connect to the correct machine ssh app01 cat opt/simple-webapp-flask/app.py ########## Output Snippet ########## if __name__ == \"__main__\" : app.run ( host = \"0.0.0.0\" , port = 8080 ) ########## End Snippet ########## Exercise 3 In app01 server ensure that our app in /opt/simple-webapp-flask/ run on port 5000 and listen to 127.0.0.1 IP. Run python app in background with nohup Note: Nohup stands for no hang up, which can be executed as shown below. nohup command-with-options & Adding \"&\" at the end will move the process to run in background. When you execute a Unix job in the background ( using &) and logout from the session, your process will get killed. You can avoid this with nohup. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Modify the app.py file vi app.py # Before modification -- REMEMBER THE QUOTES!!!! if __name__ == \"__main__\" : app.run ( host = \"0.0.0.0\" , port = 8080 ) # After modification if __name__ == \"__main__\" : app.run ( port = 5000 ) # Alternatively sudo sed -i 's/0.0.0.0/127.0.0.1/g;s/8080/5000/g' /opt/simple-webapp-flask/app.py # Run python app in background with nohup cd /opt/simple-webapp-flask nohup python app.py & Exercise 4 From app01 server is apache tomcat of host01 accessible? 1 2 3 4 # Check if available curl host01:9090 # It was available since apache tomcat was started on all IP interfaces Exercise 5 From host01 server is Python flask of app01 server accessible? 1 2 3 4 # It shouldn't be available since we changed the ip interface to 127.0.0.1 curl app01:5000 # As expected, it is not available Exercise 6 Enable Flask app to be reachable from host01 and confirm by browser tab To make app reachable from outside we have to bind app to global IP that will be used to access it or simply listen on all interfaces. In this case please make Flask app listen on all IP interfaces 1 2 3 4 5 6 7 8 9 # Make the flask app listen on all IP interfaces ssh app01 sudo sed -i 's/127.0.0.1/0.0.0.0/g' /opt/simple-webapp-flask/app.py # Restart flask app pkill python cd /opt/simple-webapp-flask/ nohup python app.py & Exercise 7 We added app01 IP address 172.16.239.30 to flask application but seems curl app01:5000 not working on host01 . Please fix the issue so that curl app01:5000 works as well as curl 172.16.239.30:5000 works on host01 1 2 3 4 5 6 7 # Change app.py to be open to all ip addresses sudo sed -i 's/172.16.239.30/0.0.0.0/g' /opt/simple-webapp-flask/app.py # Restart flask app sudo pkill python cd /opt/simple-webapp-flask/ nohup python app.py &","title":"Ip addresses and Ports from a Web Applciation Perspective"},{"location":"webservers/IPs_and_ports/#ip-addresses-and-ports-from-a-web-applciation-perspective","text":"","title":"Ip addresses and Ports from a Web Applciation Perspective"},{"location":"webservers/IPs_and_ports/#faq","text":"What IP address and port should be used? localhost vs 127.0.0.1 vs IP address? Why can't I connect to my server?","title":"FAQ"},{"location":"webservers/IPs_and_ports/#ip-address-and-ports-basics","text":"Laptops, servers etc. have different kinds of interfaces and adapters for connectivity such as Wired ethernet intefaces to connect to a LAN through a hub or switch, or Wireless to connect through Wi-Fi. No matter how the device is connected, it will get an ip address assigned to the device. A device can have multiple ip addresses if they are connected to more than one adapter. For example, a laptop is both connected through ethernet and a wireless connection - illustrated below. Each of the Network Interface Cards (NIC) are divided into logical components known as ports. It's possible to have up to 65535 ports on each ip address. Each port is a communication endpoint. Programs can listen on these ports for requests. For example, a flask application listens on port 5000 by default, but can be configured in the code. If the device has multiple ip addresses the configuration for listening on the correct one also has to be configured. The following example illustrates the point. 1 2 3 4 5 6 7 8 9 10 11 # main.py from flask import Flask app = Flask ( __name__ ) @app . route ( '/' ) def hello (): return 'Hello World!' if __name__ == '__main__' : app . run ( port = 8000 , host = 10.0.2.15 ) The example makes the application available to a single ip address but it is also possible for both ip addresses ( 10.0.2.15 and 10.0.2.16 ) to be available for users to connect to. Instead we can specify the host to listen on all available interfaces with host=0.0.0.0 . Now the application is available to everyone to see, but if we want to just develop locally, we can simply remove the host= . When no option is specified the application will automatically listen on 127.0.0.1 (the loopback address). The loopback address is the same on all devices, but the name localhost is the same. So we can access our test site using either http://localhost:8000 or http://127.0.0.1:8000 .","title":"Ip Address and ports basics"},{"location":"webservers/IPs_and_ports/#labs","text":"","title":"Labs"},{"location":"webservers/IPs_and_ports/#exercise-1","text":"Run apache tomcat on all IP interfaces and port 9090 on host01 server. We already downloaded apache tomcat under /opt/apache-tomcat-8.5.53 directory. Modify the necessary files and start tomcat. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 cd /opt/apache-tomcat-8.5.53 cd conf > permission denied sudo chmod -R 757 /opt cd conf cat server.xml ########## Output Snippet ########## > A \"Connector\" represents an endpoint by which requests are received > and responses are returned. Documentation at : > Java HTTP Connector: /docs/config/http.html > Java AJP Connector: /docs/config/ajp.html > APR ( HTTP/AJP ) Connector: /docs/apr.html > Define a non-SSL/TLS HTTP/1.1 Connector on port 8080 > <Connector port = \"8080\" protocol = \"HTTP/1.1\" > connectionTimeout = \"20000\" > redirectPort = \"8443\" /> ########## End Snippet ########## sudo sed -i 's/8080/9090/g' apache-tomcat-8.5.53/conf/server.xml ########## Output Snippet ########## > A \"Connector\" represents an endpoint by which requests are received > and responses are returned. Documentation at : > Java HTTP Connector: /docs/config/http.html > Java AJP Connector: /docs/config/ajp.html > APR ( HTTP/AJP ) Connector: /docs/apr.html > Define a non-SSL/TLS HTTP/1.1 Connector on port 9090 > <Connector port = \"9090\" protocol = \"HTTP/1.1\" > connectionTimeout = \"20000\" > redirectPort = \"8443\" /> > A \"Connector\" using the shared thread pool > <Connector executor = \"tomcatThreadPool\" > port = \"9090\" protocol = \"HTTP/1.1\" > connectionTimeout = \"20000\" > redirectPort = \"8443\" /> ########## End Snippet ########## sudo ./apache-tomcat-8.5.53/bin/startup.sh curl localhost:9090 ; ps -ef | grep tomcat","title":"Exercise 1"},{"location":"webservers/IPs_and_ports/#exercise-2","text":"We have cloned python app repo from github in /opt/ directory in app01 server. When you start it which IP and port combination it will listen on? Python app file : /opt/simple-webapp-flask/app.py 1 2 3 4 5 6 7 8 9 10 11 # Connect to the correct machine ssh app01 cat opt/simple-webapp-flask/app.py ########## Output Snippet ########## if __name__ == \"__main__\" : app.run ( host = \"0.0.0.0\" , port = 8080 ) ########## End Snippet ##########","title":"Exercise 2"},{"location":"webservers/IPs_and_ports/#exercise-3","text":"In app01 server ensure that our app in /opt/simple-webapp-flask/ run on port 5000 and listen to 127.0.0.1 IP. Run python app in background with nohup Note: Nohup stands for no hang up, which can be executed as shown below. nohup command-with-options & Adding \"&\" at the end will move the process to run in background. When you execute a Unix job in the background ( using &) and logout from the session, your process will get killed. You can avoid this with nohup. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Modify the app.py file vi app.py # Before modification -- REMEMBER THE QUOTES!!!! if __name__ == \"__main__\" : app.run ( host = \"0.0.0.0\" , port = 8080 ) # After modification if __name__ == \"__main__\" : app.run ( port = 5000 ) # Alternatively sudo sed -i 's/0.0.0.0/127.0.0.1/g;s/8080/5000/g' /opt/simple-webapp-flask/app.py # Run python app in background with nohup cd /opt/simple-webapp-flask nohup python app.py &","title":"Exercise 3"},{"location":"webservers/IPs_and_ports/#exercise-4","text":"From app01 server is apache tomcat of host01 accessible? 1 2 3 4 # Check if available curl host01:9090 # It was available since apache tomcat was started on all IP interfaces","title":"Exercise 4"},{"location":"webservers/IPs_and_ports/#exercise-5","text":"From host01 server is Python flask of app01 server accessible? 1 2 3 4 # It shouldn't be available since we changed the ip interface to 127.0.0.1 curl app01:5000 # As expected, it is not available","title":"Exercise 5"},{"location":"webservers/IPs_and_ports/#exercise-6","text":"Enable Flask app to be reachable from host01 and confirm by browser tab To make app reachable from outside we have to bind app to global IP that will be used to access it or simply listen on all interfaces. In this case please make Flask app listen on all IP interfaces 1 2 3 4 5 6 7 8 9 # Make the flask app listen on all IP interfaces ssh app01 sudo sed -i 's/127.0.0.1/0.0.0.0/g' /opt/simple-webapp-flask/app.py # Restart flask app pkill python cd /opt/simple-webapp-flask/ nohup python app.py &","title":"Exercise 6"},{"location":"webservers/IPs_and_ports/#exercise-7","text":"We added app01 IP address 172.16.239.30 to flask application but seems curl app01:5000 not working on host01 . Please fix the issue so that curl app01:5000 works as well as curl 172.16.239.30:5000 works on host01 1 2 3 4 5 6 7 # Change app.py to be open to all ip addresses sudo sed -i 's/172.16.239.30/0.0.0.0/g' /opt/simple-webapp-flask/app.py # Restart flask app sudo pkill python cd /opt/simple-webapp-flask/ nohup python app.py &","title":"Exercise 7"},{"location":"webservers/NodeJS/","text":"NodeJS Express App See also NodeJS Express is a NodeJS framework like Flask is a framework used for Python web applications. Project setup Project structure may look as follows: 1 2 3 4 5 6 7 8 9 10 11 my-application \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.MD \u251c\u2500\u2500 package.json # Similar to requirements.txt for python \u251c\u2500\u2500 app.js # Similar to app.py or main.py for python \u251c\u2500\u2500 tests \u251c\u2500\u2500 config \u251c\u2500\u2500 routes \u251c\u2500\u2500 services \u251c\u2500\u2500 db \u2514\u2500\u2500 core The app.js contains a sample web application: 1 2 3 4 5 6 7 8 9 10 //app.js const express = require ( 'express' ) const app = express () app . get ( '/products' , ( req , res ) => res . send ( getProductList ())) app . use ( express . static ( path . join ( __dirname , 'public' ))); app . listen ( 3000 ); Run requirements first npm install . It will run the package.json (list of dependencies) and install them. Afterwards run node app.js However, it may be a that there is defined a specific database for the Dev environment, then it'll be listed in the package.json file under scripts , as the excerpt below show npm can then be used to run a specific script in the file. npm run start to run the start script. The start script has the environment variable set to production. We could also run npm run start:dev to start the development environment. When it is run, the webserver is listening on localhost http://localhost:3000/ . Tools for running in production However, this is not considered best practice. If the application were to crash, Node will simply shut the site down, preventing access to the application for other users. supervisord forever pm2 (P(rocess) M(anager) 2) pm2 is used in the example. It has built in load balancers. To run our app with pm2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # start the application pm2 start app.js # start the application with multiple workers (cluster mode) pm2 start app.js -i 4 ### quickstart commands #Start and Daemonize any application: pm2 start app.js #Load Balance 4 instances of api.js: pm2 start api.js -i 4 #Monitor in production: pm2 monitor #Make pm2 auto-boot at server restart: pm2 startup","title":"NodeJS Express App"},{"location":"webservers/NodeJS/#nodejs-express-app","text":"See also NodeJS Express is a NodeJS framework like Flask is a framework used for Python web applications.","title":"NodeJS Express App"},{"location":"webservers/NodeJS/#project-setup","text":"Project structure may look as follows: 1 2 3 4 5 6 7 8 9 10 11 my-application \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.MD \u251c\u2500\u2500 package.json # Similar to requirements.txt for python \u251c\u2500\u2500 app.js # Similar to app.py or main.py for python \u251c\u2500\u2500 tests \u251c\u2500\u2500 config \u251c\u2500\u2500 routes \u251c\u2500\u2500 services \u251c\u2500\u2500 db \u2514\u2500\u2500 core The app.js contains a sample web application: 1 2 3 4 5 6 7 8 9 10 //app.js const express = require ( 'express' ) const app = express () app . get ( '/products' , ( req , res ) => res . send ( getProductList ())) app . use ( express . static ( path . join ( __dirname , 'public' ))); app . listen ( 3000 ); Run requirements first npm install . It will run the package.json (list of dependencies) and install them. Afterwards run node app.js However, it may be a that there is defined a specific database for the Dev environment, then it'll be listed in the package.json file under scripts , as the excerpt below show npm can then be used to run a specific script in the file. npm run start to run the start script. The start script has the environment variable set to production. We could also run npm run start:dev to start the development environment. When it is run, the webserver is listening on localhost http://localhost:3000/ .","title":"Project setup"},{"location":"webservers/NodeJS/#tools-for-running-in-production","text":"However, this is not considered best practice. If the application were to crash, Node will simply shut the site down, preventing access to the application for other users. supervisord forever pm2 (P(rocess) M(anager) 2) pm2 is used in the example. It has built in load balancers. To run our app with pm2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # start the application pm2 start app.js # start the application with multiple workers (cluster mode) pm2 start app.js -i 4 ### quickstart commands #Start and Daemonize any application: pm2 start app.js #Load Balance 4 instances of api.js: pm2 start api.js -i 4 #Monitor in production: pm2 monitor #Make pm2 auto-boot at server restart: pm2 startup","title":"Tools for running in production"},{"location":"webservers/Python-flask/","text":"Python Flask Web App Project setup Project structure may look as follows: 1 2 3 4 5 6 7 8 9 10 11 12 my-application \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.MD \u251c\u2500\u2500 requirements.txt # Similar to package.json NodeJS \u251c\u2500\u2500 main.py # Similar to app.js for NodeJS \u251c\u2500\u2500 utils \u251c\u2500\u2500 tests \u251c\u2500\u2500 config \u251c\u2500\u2500 routes \u251c\u2500\u2500 services \u251c\u2500\u2500 db \u2514\u2500\u2500 core The main.py contains a sample web application: 1 2 3 4 5 6 7 8 9 10 11 # main.py from flask import Flask app = Flask ( __name__ ) @app . route ( '/' ) def hello (): return 'Hello World!' if __name__ == '__main__' : app . run () Run requirements first pip install -r requirements.txt Afterwards run python main.py When it is run, the webserver is listening on localhost http://127.0.0.1:5000/ . This is not production ready. Tools for running in production Tools used for running Flask in production are: Gunicorn (default port 8000) uWSGI Gevent Twisted Web These are production grade webservers used to host multiple types of applications. To run our app in Gunicorn, we can run specific Gunicorn commands. 1 2 3 4 5 6 # main == file name # app == app variable in application file gunicorn main : app # Spawning more workers (processes) gunicorn main : app - w 2 Lab exercise \"Change description in app.py.\" 1 sudo sed -i 's/8080/5000/g' app.py \"Stop previously running app and run app.py with Gunicorn\" 1 2 # Run with single worker sudo pip install gunicorn ; gunicorn app:app \"Run Gunicorn with 3 workers in background and confirm with url\" 1 2 3 4 5 6 7 nohup gunicorn app:app -w 3 & and curl localhost:8000 # notes on nohup # Nohup is short for \u201cNo Hangups.\u201d It's not a command # that you run by itself. Nohup is a supplemental # command that tells the Linux system not to stop # another command once it has started.","title":"Python Flask Web App"},{"location":"webservers/Python-flask/#python-flask-web-app","text":"","title":"Python Flask Web App"},{"location":"webservers/Python-flask/#project-setup","text":"Project structure may look as follows: 1 2 3 4 5 6 7 8 9 10 11 12 my-application \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.MD \u251c\u2500\u2500 requirements.txt # Similar to package.json NodeJS \u251c\u2500\u2500 main.py # Similar to app.js for NodeJS \u251c\u2500\u2500 utils \u251c\u2500\u2500 tests \u251c\u2500\u2500 config \u251c\u2500\u2500 routes \u251c\u2500\u2500 services \u251c\u2500\u2500 db \u2514\u2500\u2500 core The main.py contains a sample web application: 1 2 3 4 5 6 7 8 9 10 11 # main.py from flask import Flask app = Flask ( __name__ ) @app . route ( '/' ) def hello (): return 'Hello World!' if __name__ == '__main__' : app . run () Run requirements first pip install -r requirements.txt Afterwards run python main.py When it is run, the webserver is listening on localhost http://127.0.0.1:5000/ . This is not production ready.","title":"Project setup"},{"location":"webservers/Python-flask/#tools-for-running-in-production","text":"Tools used for running Flask in production are: Gunicorn (default port 8000) uWSGI Gevent Twisted Web These are production grade webservers used to host multiple types of applications. To run our app in Gunicorn, we can run specific Gunicorn commands. 1 2 3 4 5 6 # main == file name # app == app variable in application file gunicorn main : app # Spawning more workers (processes) gunicorn main : app - w 2","title":"Tools for running in production"},{"location":"webservers/Python-flask/#lab-exercise","text":"\"Change description in app.py.\" 1 sudo sed -i 's/8080/5000/g' app.py \"Stop previously running app and run app.py with Gunicorn\" 1 2 # Run with single worker sudo pip install gunicorn ; gunicorn app:app \"Run Gunicorn with 3 workers in background and confirm with url\" 1 2 3 4 5 6 7 nohup gunicorn app:app -w 3 & and curl localhost:8000 # notes on nohup # Nohup is short for \u201cNo Hangups.\u201d It's not a command # that you run by itself. Nohup is a supplemental # command that tells the Linux system not to stop # another command once it has started.","title":"Lab exercise"}]}